

# LM-DiskANN DuckDB Extension Progress - YYYY-MM-DD (Shadow Proposal Update)

This document reflects the project status and upcoming work based on the adoption of the "Shadow Lookup Table" design proposed on 2025-05-06. This new architecture significantly changes the persistence and transactional handling of the index.

**I. Foundational Work Completed (Largely from Previous Design Iteration):**

*   **[X] Code Refactoring & Structure:**
    *   Modular C++ structure (`config`, `node`, `storage`, `search`, `distance`, `state`, `index` modules initiated).
    *   Header files define initial interfaces.
*   **[X] Configuration & Initial Layout (`lm_diskann_config`):**
    *   Parameter parsing from `CREATE INDEX ... WITH (...)` options implemented (`ParseOptions`).
    *   Parameter validation logic implemented (`ValidateParameters`).
    *   Initial Node block layout concepts (`NodeLayoutOffsets`) explored, previously assuming Ternary neighbor representation. *(This will need adaptation for the new `NodeBlock` struct).*
    *   Support for `FLOAT32` and `INT8` node vectors initiated.
*   **[X] Low-Level Node Access Concepts (`lm_diskann_node`):**
    *   Conceptual accessor functions for reading/writing block components were designed. *(To be heavily revised for the new `NodeBlock` and I/O via `FileSystem`)*.
*   **[X] Distance Functions (`lm_diskann_distance`):**
    *   Implementations for Cosine and Inner Product distances (FLOAT32, INT8 via ConvertToFloat).
    *   `CalculateApproxDistance` (previously Ternary-based) and `CompressVectorForEdge` (previously Ternary-based) exist but may need adaptation if neighbor compression changes.
*   **[X] Core Index Class Structure (`LMDiskannIndex`):**
    *   Inherits from `duckdb::BoundIndex`.
    *   Uses `LMDiskannConfig` struct for parameters.
    *   Initial constructor flow for options parsing, validation.
    *   Placeholders for `BoundIndex` virtual method overrides.
*   **[~] Search Algorithm Structure (`lm_diskann_search`):**
    *   `PerformSearch` function implementing beam search loop is structured. *(Will need significant changes to integrate with new caching, shadow lookup, and `commit_epoch` visibility).*
*   **[~] Initial Core Operations (`lm_diskann_index`):**
    *   `Insert` (conceptual: allocate node, init block, write vector, find neighbors).
    *   `FindAndConnectNeighbors` / `RobustPrune` (conceptual).
    *   `Delete` (conceptual: map removal).
    *   `InitializeScan` / `Scan` (conceptual).
    *(All core operations require substantial rework for the new architecture).*

**II. New Architectural Design Adopted: "Shadow Lookup Table"**

The project has pivoted to the "Shadow Lookup Table" architecture. Key features:

*   **Folder-per-Index:** Each index resides in its own directory.
*   **Primary Graph File (`graph.lmd`):** Append-only file for fixed-size `NodeBlock`s (vector, graph links, internal transactional headers). Managed via DuckDB's `FileSystem` API.
*   **Shadow Delta Store (`shadow.duckdb`):** An embedded DuckDB database (WAL-enabled) storing modified/new `NodeBlock` data (`__lmd_blocks` table) before merging.
*   **Lookup Mapping Store (`lookup.duckdb`):** A separate embedded DuckDB database (WAL-enabled) mapping base table `row_id` to index `node_id` with ACID properties.
*   **Metadata File (`graph.lmd.meta`):** Stores header info for `graph.lmd`.
*   **Transactional Consistency (MVCC-like):** `NodeBlock`s contain `txn_id` and `commit_epoch` for visibility control, integrated with DuckDB's transaction lifecycle.
*   **In-Memory LRU Cache:** For `NodeBlock`s.
*   **Operational Flow:**
    *   **Writes:** Updates go to an in-memory ring buffer, then flushed by a daemon to `shadow.duckdb`.
    *   **Reads:** Check LRU cache, then `shadow.duckdb`, then `graph.lmd`.
    *   **Merge/Compaction:** Periodically, or on checkpoint, data from `shadow.duckdb` is merged into `graph.lmd`.

**III. Remaining Work & Next Steps (Based on New "Shadow Proposal" Design):**

*(Highest Priority Architectural Implementation)*

1.  **Folder and File Management System:**
    *   **Task:** Implement logic to create/manage the dedicated directory for each LM-DiskANN index. This includes initializing `graph.lmd`, `graph.lmd.meta`, and the two embedded DuckDB databases (`shadow.duckdb`, `lookup.duckdb`).
    *   Use DuckDB's `FileSystem` API for all directory and file operations.
    *   Ensure `shadow.duckdb` and `lookup.duckdb` are opened/created with WAL mode enabled.
    *   **Challenge:** Robust path management, error handling for file system operations.
2.  **Embedded DuckDB Integration (`shadow.duckdb`, `lookup.duckdb`):**
    *   **Task:** Integrate embedded DuckDB instances.
        *   For `shadow.duckdb`: Create the `__lmd_blocks (block_id BIGINT PK, data BLOB, version BIGINT, checksum BIGINT)` table. Implement helper functions to interact with it using prepared statements (Insert/Replace, Select by ID, Select Range for Merge, Clear All).
        *   For `lookup.duckdb`: Create the `lmd_lookup (row_id BIGINT PK, node_id BIGINT)` table. Implement helper functions for Insert, Delete by `row_id`, Select by `row_id`, Select by `node_id`.
    *   **Challenge:** Managing DuckDB instance lifecycles, connection handling, efficient prepared statement usage.
3.  **`NodeBlock` Definition and I/O:**
    *   **Task:** Define the C++ `NodeBlock` struct, including fields for `id`, full vector, neighbor list (IDs + compressed vectors), `txn_id`, `commit_epoch`, `node_version`, `checksum`, and padding to fixed size (e.g., 8KB).
    *   Implement serialization/deserialization for `NodeBlock` to/from a `char*` buffer.
    *   Implement low-level I/O for reading/writing `NodeBlock`s from/to `graph.lmd` via `FileSystem::Read/Write` at specific offsets.
    *   **Challenge:** Efficient (de)serialization, correct packing, checksum validation.
4.  **In-Memory LRU Cache & Ring Buffer:**
    *   **Task:** Implement the LRU cache for `NodeBlock` objects (keyed by `node_id`).
    *   Implement the lock-free ring buffer for `DirtyBlockEntry` (block\_id, data\_ptr, checksum) to stage blocks for the flush daemon.
    *   **Challenge:** Thread-safe cache implementation, efficient ring buffer, cache eviction policy.
5.  **Metadata File (`graph.lmd.meta`):**
    *   **Task:** Define format and implement read/write for `graph.lmd.meta` (e.g., dimensions, block size, total node count, free list head, consistency markers for `graph.lmd`).
    *   **Challenge:** Ensuring atomicity/consistency of meta file updates, especially after merges.

*(Core Algorithm & Operational Flow Implementation)*

6.  **Flush Daemon:**
    *   **Task:** Implement the background flush thread.
        *   Pops entries from the dirty ring buffer.
        *   Deduplicates entries (last write wins for a given block\_id).
        *   Batches entries and inserts/replaces them into `shadow.duckdb`.`__lmd_blocks` using prepared statements within a transaction.
        *   Triggered periodically, by ring buffer fullness, or potentially on transaction commit hints.
    *   **Challenge:** Efficient batching, error handling, coordination with merge process.
7.  **Merge/Compaction Process:**
    *   **Task:** Implement the merge logic.
        *   Reads blocks from `shadow.duckdb`.`__lmd_blocks` (ordered by `block_id`).
        *   Writes these blocks to their correct offsets in `graph.lmd`.
        *   Handles appending new blocks vs. overwriting existing ones.
        *   After successful write and `fsync` of `graph.lmd`, clears the processed entries from `__lmd_blocks`.
        *   Updates `graph.lmd.meta`.
        *   Triggered by shadow table size or DuckDB checkpoint events.
    *   **Challenge:** Efficient batched writes to `graph.lmd`, ensuring atomicity/crash safety of the merge itself, updating `graph.lmd.meta` consistently.
8.  **Insertion Logic (Major Rework):**
    *   **Task:** Adapt `LMDiskannIndex::Insert`.
        *   Allocate new `node_id`.
        *   Construct `NodeBlock` with vector, `txn_id` (current transaction), provisional `commit_epoch`.
        *   Perform neighbor search (Robust Prune) considering existing nodes (from cache/shadow/graph.lmd) and their visibility.
        *   Update neighbor lists of affected nodes (creating new versions of their `NodeBlock`s).
        *   Add new/updated `NodeBlock`s to LRU cache, mark as dirty.
        *   Insert `(row_id, new_node_id)` into `lookup.duckdb`.`lmd_lookup` within the user's transaction.
        *   Push dirty `NodeBlock` entries to the ring buffer.
        *   On transaction commit, finalize `commit_epoch` for new/updated blocks.
    *   **Challenge:** Correct `commit_epoch` handling, robust neighbor updates (copy-on-write), efficient interaction with cache and ring buffer.
9.  **Search Logic (Major Rework):**
    *   **Task:** Adapt `LMDiskannIndex::Scan` / `PerformSearch`.
        *   Graph traversal logic needs to fetch `NodeBlock`s via a `readNodeBlock(id, current_txn_snapshot_epoch)` helper.
        *   `readNodeBlock` must: 1. Check LRU cache. 2. Check `shadow.duckdb`.`__lmd_blocks`. 3. Read from `graph.lmd`.
        *   Crucially, `readNodeBlock` must verify the `commit_epoch` of the fetched block against the querying transaction's snapshot epoch to ensure MVCC visibility. Skip invisible nodes/neighbors.
        *   Map resulting `node_id`s to `row_id`s via `lookup.duckdb`.
    *   **Challenge:** Correct `commit_epoch` visibility checks, efficient multi-source block retrieval, performance under concurrent reads/writes.
10. **Deletion Logic (Major Rework):**
    *   **Task:** Adapt `LMDiskannIndex::Delete`.
        *   Lookup `node_id` from `row_id` via `lookup.duckdb`.
        *   Mark the `NodeBlock` as deleted (e.g., tombstone flag, special `commit_epoch`).
        *   Update affected neighbor `NodeBlock`s to remove the deleted node from their lists.
        *   Add the deleted `NodeBlock` (tombstone) and updated neighbor `NodeBlock`s to cache/ring buffer.
        *   Delete entry from `lookup.duckdb`.`lmd_lookup`.
        *   Handle actual space reclamation in `graph.lmd` during merge/vacuum.
    *   **Challenge:** Ensuring neighbor lists are correctly updated, handling tombstones.

*(Robustness, Maintenance & Integration)*

11. **Transactional Consistency & MVCC:**
    *   **Task:** Implement robust `commit_epoch` generation (e.g., using a global atomic counter tied to DuckDB transaction commits if possible, or a high-water mark). Ensure all node accesses respect MVCC.
    *   Handle transaction rollback: prevent flushed blocks from aborted transactions from becoming visible, or purge them.
    *   **Challenge:** Deep integration with DuckDB's transaction lifecycle. Accurately capturing commit epochs.
12. **Concurrency Control:**
    *   **Task:** Design and implement locking mechanisms for:
        *   LRU Cache.
        *   Ring Buffer.
        *   Updates to `NodeBlock` neighbor lists (e.g., sharded locks).
        *   Merge process.
    *   **Challenge:** Ensuring correctness and avoiding deadlocks with acceptable performance.
13. **Vacuuming / Space Reclamation:**
    *   **Task:** Design and implement a `VACUUM` operation for `graph.lmd` to reclaim space from deleted/old versioned blocks and potentially compact the file.
    *   Maintain a free list within `graph.lmd.meta` or `graph.lmd` itself.
    *   **Challenge:** Complex file manipulation, ensuring consistency during vacuum.
14. **Testing Strategy:**
    *   **Task:** Develop comprehensive unit and integration tests focusing on:
        *   Correctness of insert, delete, search under new architecture.
        *   Transactional consistency (visibility, rollback).
        *   Crash recovery scenarios (during flush, during merge).
        *   Concurrency.
    *   **Challenge:** Simulating crash scenarios effectively. Testing MVCC properties.
15. **DuckDB Index API Integration:**
    *   **Task:** Ensure all necessary `BoundIndex` virtual methods (`Append`, `Delete`, `CommitDrop`, `Vacuum`, `GetStorageInfo`, `Serialize`, `Deserialize`) are correctly implemented or adapted for the new design.
        *   `Serialize`/`Deserialize` will now handle `graph.lmd.meta` content (path to folder, overall graph stats, *not* block data).
    *   **Challenge:** Mapping the new design's concepts to the existing `BoundIndex` API.

This updated plan reflects the significant undertaking of implementing the shadow proposal. The focus shifts from direct block management within DuckDB's buffer system to a more sophisticated external file and embedded database approach, aiming for better scalability, update performance, and transactional integrity.
