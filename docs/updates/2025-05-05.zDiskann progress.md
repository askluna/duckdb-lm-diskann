# LM-DiskANN DuckDB Extension Progress - YYYY-MM-DD

Based on the refactoring work as of YYYY-MM-DD.

**I. Work Completed:**

*   **[X] Code Refactoring & Structure:**
    *   Modular C++ structure (`config`, `node`, `storage`, `search`, `distance`, `state`, `index`).
    *   Header files define interfaces.
*   **[X] Configuration & Layout (`lm_diskann_config`):**
    *   Parameter parsing from `CREATE INDEX ... WITH (...)` options implemented (`ParseOptions`).
    *   Parameter validation logic implemented (`ValidateParameters`).
    *   Node block layout (`NodeLayoutOffsets`) calculation implemented (`CalculateLayoutInternal`), assuming **implicit Ternary** neighbor representation.
    *   Support only `FLOAT32` and `INT8` node vectors.
*   **[X] Node Block Access (`lm_diskann_node`):**
    *   Low-level accessor functions for reading/writing block components based on `NodeLayoutOffsets`.
*   **[X] Distance Functions (`lm_diskann_distance`):**
    *   Implementations for Cosine and Inner Product distances (FLOAT32, INT8 via ConvertToFloat). *L2 removed due to incompatibility with Ternary assumption.*
    *   `CalculateApproxDistance` implemented (assumes Ternary neighbors, uses ternary dot product proxy).
    *   `CompressVectorForEdge` implemented (assumes Ternary output).
*   **[X] Core Index Class (`LmDiskannIndex`):**
    *   Inherits from `duckdb::BoundIndex`.
    *   Uses `LmDiskannConfig` struct for parameters.
    *   Uses `LmDiskannDBState` struct for DuckDB integration objects.
    *   Constructor handles options parsing, validation, layout, allocator init, load/init flow.
    *   Overrides for `BoundIndex` virtual methods delegate to helpers/modules.
*   **[X] Search Algorithm (`lm_diskann_search`):**
    *   `PerformSearch` function implementing beam search loop is structured.
*   **[X] Core Operations (`lm_diskann_index`):**
    *   `Insert`: Allocates node, initializes block, writes vector, calls `FindAndConnectNeighbors`.
    *   `FindAndConnectNeighbors`: Implemented search, calls `RobustPrune`.
    *   `RobustPrune`: Implemented core logic (gather, sort, prune, update node).
    *   `Delete`: Implemented map removal/block freeing (in-memory map) and queuing.
    *   `InitializeScan` / `Scan`: Implemented using entry point logic and `PerformSearch`.
*   **[X] Metadata Persistence (`lm_diskann_storage`, `lm_diskann_index`):**
    *   `PersistMetadata` / `LoadMetadata` implemented for core parameters, entry point, delete queue head. *Does not persist edge type (implicitly Ternary)*.
*   **[~] In-Memory Placeholders:** *(Partially addressed by struct refactor, but core limitations remain)*
    *   **RowID Mapping:** Uses `std::map` (`TryGetNodePointer`, `AllocateNode`, `DeleteNodeFromMapAndFreeBlock`). **NEEDS ART.**
    *   **Entry Point:** Uses `std::map` (`GetEntryPoint`, `SetEntryPoint`, `GetRandomNodeID`). **NEEDS ART.**
    *   **Delete Queue:** Uses main allocator (`EnqueueDeletion`). **NEEDS PROCESSING LOGIC.**

**II. Remaining Work & Next Steps:**

*(Highest Priority Blockers)*

1.  **Persistent RowID Mapping (ART Integration):**
    *   **Task:** Replace `std::map` with `unique_ptr<ART>`. Implement storage helpers using ART API. Persist/load ART root pointer. Handle `row_t` keys correctly.
    *   **Challenge:** ART API, key serialization, allocator integration, concurrency.
2.  **Delete Queue Processing:**
    *   **Task:** Implement logic in `ProcessDeletionQueue` (called by `Vacuum`). Read queue, find referring nodes (via ART iteration?), remove deleted edges from neighbors.
    *   **Challenge:** Efficiently finding referring nodes, graph consistency during updates.

*(Core Functionality & Refinements)*

1.  **Robust Pruning Optimization:**
    *   **Task:** Optimize distance calculations (avoid redundant reads/conversions).
    *   **Challenge:** Balancing accuracy and performance.
2.  **Search Duplicate Check:**
    *   **Task:** Efficiently avoid adding duplicate nodes to candidate queue in `PerformSearch`.
    *   **Challenge:** Overhead of check vs. benefit.
3.  **Entry Point Refinement (Requires ART):**
    *   **Task:** Implement Pointer -> RowID lookup (maybe store RowID in node?). Implement efficient `GetRandomNodeID` using ART sampling/iteration.
    *   **Challenge:** ART API, potential layout changes.
4.  **Distance/Compression (INT8):**
    *   **Task:** Acknowledge `ConvertToFloat` for INT8 loses precision. True INT8 distance/search might need scale/offset (not planned).
    *   **Challenge:** Accuracy vs. complexity trade-off.
5.  **Filtering / Scan Enhancements:**
    *   **Task:** Ensure `InitializeScan` / `Scan` can handle large `k` efficiently for post-filtering strategies.
    *   **Task:** *Consider* adding support for threshold-based scans (return all neighbors within a distance).
    *   **Task:** Document challenges of pre-filtering (candidate RowIDs) with graph indexes.

*(Robustness & Production Readiness)*

1.  **Testing:**
    *   **Task:** Develop comprehensive unit and integration tests.
    *   **Challenge:** Test cases for graph consistency, concurrency, edge cases.
2.  **Concurrency Control:**
    *   **Task:** Add `IndexLock` usage around critical sections modifying shared state (ART, allocator, node blocks).
    *   **Challenge:** Identifying critical sections, deadlock avoidance.
3.  **Error Handling & Rollback:**
    *   **Task:** Improve cleanup scenarios (e.g., `Insert` failing after allocation).
    *   **Challenge:** Atomicity guarantees.
4.  **Linter/Build Fixes:**
    *   **Task:** Address remaining build errors stemming from API mismatches identified during analysis (e.g., `IndexStorageInfo` fields, `BufferHandle::SetModified` replacement).
    *   **Challenge:** Requires checking current DuckDB source/API.
5.  **Delete Queue Allocator:**
    *   **Task:** *Consider* separate allocator for delete queue entries.
    *   **Challenge:** Managing second allocator.

Considerations:
- Consider vector itnernals and how my implementation is impacted https://duckdb.org/docs/stable/internals/vector
- memory management here:  https://duckdb.org/2024/03/29/external-aggregation.html
https://hannes.muehleisen.org/publications/icde2024-out-of-core-kuiper-boncz-muehleisen.pdf


**Explaining implementation choices**

First, I should clarify the metric selection for index creation, mentioning vector transformations like L2 or cosine, and possibly the need to duplicate indexes depending on the metric. For block packing, I should aim for a simplified explanation, maybe referencing ternary quantization. Regarding queue and parallel graph builds, I'll explain LM-DiskANN or DiskANN parallelization, highlighting trade-offs and complexity. Finally, vacuum heuristics: I’ll provide clear plans for triggers and background tasks while addressing how to manually control it and the complexity of adding automatic heuristics. Don't forget shared code info from config.hpp!


**Providing guidance on index creation and packing**

For metric selection, we should specify the metric at index creation, and normalize vectors when needed, such as for cosine similarity. If the metric is cosine, we could optionally auto-normalize vectors on insert. For block packing, I'll simplify the explanation by advising to compute node size, then choose the appropriate block size (4KB if the node size is ≤ 4096, otherwise 8KB). No packing for two nodes—I'll refer to ternary quantization for further understanding.

**Analyzing parallel graph build and vacuum heuristics**

For parallel graph building, the decision between queue and single-thread vs. multi-thread (as in LM-DiskANN) depends on balancing concurrency with complexity. Multi-threading offers batch processing with locks, while single-thread simplifies testing and concurrency control. I recommend starting with a single-thread queue and introducing a batch insertion worker.

For vacuum heuristics, I suggest combining manual PRAGMA with background automatic vacuum tasks, and setting thresholds for optimization. Task scheduling can handle complexity, but the ternary quantization file isn't needed to proceed immediately. Citations will cover metrics and normalization details.


**Analyzing parallel graph build and vacuum heuristics**

For the parallel graph build, the decision between using a queue with a single thread or multi-threaded updates should consider trade-offs. Multi-threading offers parallelism with batch processing, but adds complexity and concurrency control challenges. I suggest starting with a single-threaded queue and potentially using a batch insertion worker for simplicity.

For vacuum heuristics, consider a mix of manual vacuum tasks alongside background auto-vacuuming and thresholds. Implementing background jobs can introduce complexity, but TaskScheduler could help manage it more effectively.


Thought for 43 seconds


---

# Response

Below is a **drill‑down on the four open items**—metric selection, block layout choice, parallel graph build, and automatic vs manual vacuum—grounded in the code you shared (`config.hpp`, `node.cpp`) and the latest LM‑DiskANN literature.

---

## 1 . Metric must be fixed at **index‑creation time**

### Why

* The distance metric dictates **both the math and the data**: for cosine you need vectors already L2‑normalised (‖v‖₂ ≈ 1), otherwise the cosine and inner‑product results diverge.([Stack Overflow][1], [PostgresML][2])
* Mixing metrics inside one index corrupts routing—edge weights that look short in L2 may be long in cosine space and vice‑versa.([Milvus][3])

### How in your code

* `LmDiskannMetricType` is already in `config.hpp`; extend `ParseOptions()` to **require** `WITH (metric='cosine' | 'l2' | …)` at `CREATE INDEX`.
* At build time

  ```cpp
  if (config.metric_type == COSINE)
      NormalizeInPlace(vector);   // scale to unit length once
  ```

  store the **normalised** values inside node blocks; searches simply reuse them.
* Allow multiple indexes on the same column by adding `metric_type` to the catalog key `(table_id, column_id, metric)`. Similar precedent exists in Milvus and VSS (`metric_type` per‑index).([Milvus][4], [Milvus][5])

---

## 2 . Block size & packing—keep it **simple and safe**

### Compute the real node footprint

Using your ternary helper (`WordsPerPlane(dim)`):

```
bytes_per_neighbor =
    8             // rowID
  + 2 * WordsPerPlane(dim) * 8;
node_size ≈ 2 + dim*sizeof(float) + R * bytes_per_neighbor;
```

With the default config (`dim=128`, `R=64`) this lands around **5.8 kB**.

### Guideline

1. Pick the **smallest power‑of‑two page** that fits one node.

   * `≤ 4 kB` → choose 4 kB.
   * Otherwise bump once → **8 kB**.
2. **Do *not* pack two nodes** into one 8 kB page; the pointer maths, alignment, and partial‑write handling increase code‑surface for <10 % throughput gain on NVMe (random read latency dominates).
3. Put `block_size_bytes` in metadata (`LmDiskannMetadata`) so future compaction tools can up‑size later.

This keeps allocator logic and `node.cpp` accessors exactly as written—no inner‑offset table, no variable‑length arrays.

---

## 3 . Parallel graph build: queue‑worker vs full concurrency

| Build mode                                            | Throughput                                  | Implementation cost                                                                    | Determinism & testability                     | Papers / repos                                               |
| ----------------------------------------------------- | ------------------------------------------- | -------------------------------------------------------------------------------------- | --------------------------------------------- | ------------------------------------------------------------ |
| **Single thread, FIFO queue**                         | \~2–3 M insert/s for 256‑dim  on 1 CPU core | trivial (your existing `Insert` path)                                                  | deterministic; golden‑file tests easy         | baseline LM‑DiskANN code ([School of Computing][6])          |
| **Batch‑parallel, shared neighbor lists**             | 5‑8× on 16 cores (ParlayANN, cuVS)          | needs per‑node spin‑lock or `std::atomic<uint16_t> degree`; careful SSD write batching | non‑deterministic order ⇒ harder diff‑testing | ParlayANN ([Terpconnect][7]), cuVS Vamana ([RAPIDS Docs][8]) |
| **Fully concurrent streaming updates (FreshDiskANN)** | similar to batch but supports live ingest   | extra SSD contention; complex merge & patch queues                                     | high complexity; race conditions tricky       | FreshDiskANN ([arXiv][9])                                    |

### Recommendation

* **Phase 1** MVP: keep the current single‑thread queue guarded by `IndexLock`.
* **Phase 2** (optional): add a `std::queue<InsertJob>` + thread‑pool; each worker holds a **fine‑grained lock per node** when mutating neighbor lists and uses `FileSystem::Write` with 256 kB async batches to amortise I/O. Limit to ≤ 4 concurrent writers to avoid SSD head‑of‑line blocking.([Stack Overflow][10])
* Unit‑test with deterministic seed insert order; replay logs under TSAN to catch data races.

---

## 4 . Vacuum heuristics—blend **auto** and **manual**

### Metrics to watch

* `delete_queue_length`
* `%_deleted = delete_queue_length / vector_count`
* `time_since_last_vacuum`

### Heuristic proposal

```text
Trigger vacuum when
   (%_deleted > 2 %) OR
   (delete_queue_length > 50_000) OR
   (now - last_vacuum > 24h)
```

*Thresholds configurable via PRAGMA.*

### Implementation steps

1. Add a **background TaskScheduler job** that wakes every `vacuum_check_interval` (default 10 min).
2. If thresholds met & no user txn active → run `ProcessDeleteQueue()` (may take minutes).
3. Expose **manual override**

   ```sql
   PRAGMA vacuum_lmdiskann('idx_name', force := true);
   ```

   This simply signals the same Task object; reuse code.

### Complexity

* Very small: the delete‑queue is already on disk; vacuum just traverses and patches neighbor nodes—logic you must implement anyway.
* Auto‑trigger adds a 20‑line check; manual path is a one‑liner that posts the task.
* Mirrors PostgreSQL’s autovacuum + `VACUUM (VERBOSE)` pattern—well‑understood by DBAs.([harsha-simhadri.org][11])

---

## 5 . Ternary quantization file integration

Your `node.cpp` accessors already compute offsets via `WordsPerPlane(dim)`. Make sure:

```cpp
layout.ternary_edge_size_bytes =
      2 * WordsPerPlane(config.dimensions) * sizeof(uint64_t);
layout.total_node_size =
      AlignValue(layout.neighbor_neg_planes_offset
                 + R * layout.ternary_edge_size_bytes, 64);
```

Expose a dev‑only `PRAGMA lmdiskann.debug_node('rowid')` that prints these offsets so block‑size errors are caught early.

---

### Final action list

| Priority | Task                                                                                     |
| -------- | ---------------------------------------------------------------------------------------- |
| P0       | Enforce `WITH(metric=...)` at `CREATE INDEX`; auto‑normalise vectors for cosine.         |
| P0       | Lock block size to **8 kB** if `node_size > 4096`; one node per block.                   |
| P1       | Keep single‑thread build; add TODO + atomic `degree` for future parallel build.          |
| P1       | Implement vacuum heuristic + `PRAGMA vacuum_lmdiskann`.                                  |
| P2       | Metrics view (`lmdiskann.stats`) with delete queue length, last vacuum time, cache hit%. |
| P2       | Benchmark parallel build branch when core is stable (ParlayANN API reference).           |

These changes give you a straight path to a functioning, testable LM‑DiskANN index while keeping complexity contained and future avenues open.

[1]: https://stackoverflow.com/questions/51290969/is-there-any-reason-to-not-l2-normalize-vectors-before-using-cosine-similarity?utm_source=chatgpt.com "Is there any reason to (not) L2-normalize vectors before using ..."
[2]: https://postgresml.org/docs/open-source/pgml/guides/embeddings/vector-normalization?utm_source=chatgpt.com "Vector Normalization – PostgresML"
[3]: https://milvus.io/blog/2021-09-24-diskann.md?utm_source=chatgpt.com "DiskANN, A Disk-based ANNS Solution with High Recall ... - Milvus"
[4]: https://milvus.io/docs/index-vector-fields.md?utm_source=chatgpt.com "Index Vector Fields | Milvus Documentation"
[5]: https://milvus.io/docs/multi-vector-search.md?utm_source=chatgpt.com "Hybrid Search | Milvus Documentation"
[6]: https://cse.unl.edu/~yu/homepage/publications/paper/2023.LM-DiskANN-Low%20Memory%20Footprint%20in%20Disk-Native%20Dynamic%20Graph-Based%20ANN%20Indexing.pdf?utm_source=chatgpt.com "[PDF] LM-DiskANN: Low Memory Footprint in Disk-Native Dynamic Graph ..."
[7]: https://www.terpconnect.umd.edu/~chutton6/slides1.pdf?utm_source=chatgpt.com "[PDF] ParlayANN: Scalable and Deterministic Parallel Graph-Based ..."
[8]: https://docs.rapids.ai/api/cuvs/stable/indexes/vamana/?utm_source=chatgpt.com "Vamana — cuvs - RAPIDS Docs"
[9]: https://arxiv.org/pdf/2105.09613?utm_source=chatgpt.com "[PDF] FreshDiskANN: A Fast and Accurate Graph-Based ANN Index for ..."
[10]: https://stackoverflow.com/questions/10254147/writing-data-to-disk-in-parallel?utm_source=chatgpt.com "Writing data to disk in parallel? - multithreading - Stack Overflow"
[11]: https://harsha-simhadri.org/pubs/Filtered-DiskANN23.pdf?utm_source=chatgpt.com "[PDF] Filtered-DiskANN: Graph Algorithms for Approximate Nearest ..."
