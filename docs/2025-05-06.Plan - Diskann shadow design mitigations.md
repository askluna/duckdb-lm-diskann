Understood. I will produce a detailed technical mitigation and implementation specification focused on the shadow architecture, including both the shadow database and the shadow lookup table mechanisms. The document will:

* Summarize the current shadow design and its key principles.
* Synthesize the full review set, including critical issues and tradeoffs.
* Propose precise mitigation strategies and technical fixes for each point.
* Provide C++ architectural guidance: API-level details, struct layout, lifecycle flow.
* Include edge case handling, performance considerations, and durability enforcement.

I'll focus entirely on technical content intended for implementers, without revisiting older pre-shadow code unless directly relevant.

I’ll begin compiling the document now and will notify you once it’s ready for review.


# LM-DiskANN Shadow Architecture – Implementation & Mitigation Specification

## Shadow System Design Overview

**Folder-Per-Index Architecture:** Each LM-DiskANN index lives in its own directory with four key components:

* **Primary Graph File (`graph.lmd`)** – An append-only file of fixed-size *node blocks* (e.g., 8KB each) storing vectors and graph links. This is the main on-disk index storing each node (vector) and its neighbors.
* **Shadow Delta Store (`shadow.duckdb`)** – A DuckDB database functioning as a WAL-backed delta table for modified blocks. Rather than updating `graph.lmd` in place for each change, new or updated node blocks are written to this shadow store transactionally, ensuring durability and atomicity for recent updates.
* **Lookup Mapping Store (`lookup.duckdb`)** – A separate DuckDB database containing a mapping table (`lmd_lookup`) from base table **row\_id** to index **node\_id**. This indirection layer allows translating ANN search results (internal node\_ids) back to user row identifiers. It is maintained with full ACID guarantees so that inserts, deletes, and rollbacks in the base table are consistently reflected.
* **Metadata File (`graph.lmd.meta`)** – A small file holding index metadata like vector dimensions, block size, node count, free list head, etc. for integrity checks and fast recovery. It may also act as a manifest/marker for the current consistent state.

**Node Block Structure:** Each fixed-size node block in `graph.lmd` is self-contained, storing one vector and its neighbor list (with neighbor IDs and often their compressed vectors). The block includes a header (with the node’s unique ID, size, version, and transactional metadata) followed by the vector’s data and a list of neighbor entries. By storing neighbors’ information within the block, graph traversal during ANN search can occur with minimal random I/O – all data needed to evaluate a node and its immediate neighbors is loaded in one block read. This slightly larger storage footprint is a trade-off to achieve **low RAM usage** and **high locality** in disk reads.

**Transactional Metadata:** To integrate with DuckDB’s MVCC, each node block carries a **commit epoch** or timestamp and an origin transaction ID. This metadata ensures that the index observes the same visibility rules as the main database. If a transaction inserts or updates a vector, its node block is tagged with the creating transaction’s ID and remains invisible to other transactions until commit. Upon commit, the block gets a commit epoch (e.g., the global commit sequence number). Readers use this to **ignore uncommitted or rolled-back nodes**: any node with a commit epoch greater than the reader’s snapshot (or a special epoch indicating invalidation) is skipped. This prevents **“ghost reads”** where an index search might otherwise return a vector that was added and rolled back, or a stale entry if a row\_id was reused after deletion. Aborted nodes (commit epoch not set due to rollback) are later garbage-collected from the shadow store.

**Durability and WAL:** By using DuckDB for the shadow delta store and lookup table, the system leverages robust WAL (Write-Ahead Logging) and recovery mechanisms without reinventing them. Any insertion into `shadow.duckdb` or `lookup.duckdb` is first logged in the DuckDB WAL, which is flushed to disk on transaction commit. This means recent index updates have the **same durability guarantees as standard DuckDB transactions** – if the system crashes, the shadow and lookup databases recover by replaying their WALs, ensuring no committed update is lost. The design avoids directly writing to `graph.lmd` except during controlled merge operations, thereby treating the shadow store as an append-only **update journal** that can be safely replayed or merged.

**Coordination of Components:** On each base table modification, the extension coordinates updates across these components to keep them in sync:

* When a new row with a vector is inserted into the base table, the extension allocates a new **node\_id** and inserts a mapping (row\_id -> node\_id) into the lookup table within the same user transaction. This ensures that if the transaction rolls back, the mapping is rolled back too (no “dangling” mappings). The actual vector and its neighbors are added as a new NodeBlock in memory and queued for flushing.
* When an existing row is deleted, the extension marks the corresponding node as deleted (tombstoned) – e.g., setting a tombstone flag or commit epoch indicating deletion – and removes its mapping from `lookup.duckdb` as part of the delete transaction. Any neighbor links referring to this node are updated (dropped) in memory and flushed so that searches won’t consider the deleted vector.
* **External ANN Index File (`graph.lmd`) Updates:** Direct writes to the main index file happen only during **merge (compaction)** operations, which occur in the background. The merge process consolidates the buffered changes from `shadow.duckdb` into the `graph.lmd` file (see the **Flush/Merge Lifecycle** below). This two-phase approach (delta then merge) isolates rapid updates from the heavy I/O of writing the large graph file. It dramatically **reduces write amplification**, since only changed blocks are rewritten to disk instead of the entire index.
* **Metadata durability:** Changes in high-level metadata (e.g., increase in node\_count, updates to free list, or a new merge “epoch”) are also made durable. For example, when a merge completes or a new block is added, the `graph.lmd.meta` file is updated. This update is done carefully (write new metadata to a temp file, fsync, then atomically rename it) to avoid corruption of the meta file on crashes. By treating the meta info similar to a mini-manifest and using atomic filesystem primitives, the system maintains a recoverable pointer to the current state of `graph.lmd`.

**Read Path (Lookup & Search):** A read (ANN query) goes through an indirection and caching sequence to ensure it sees the latest committed data:

1. **RowID to NodeID:** If the ANN query originates from a table scan (e.g., a “find nearest neighbors for row X”), the lookup table is used to translate the row\_id to the internal node\_id. For KNN searches that return many vectors, after obtaining internal node\_ids from the graph traversal, the lookup table (with an index on node\_id) translates those back to row\_ids for the final output.
2. **Cache → Shadow → Base:** Given a node\_id to fetch (either as a search entry point or during graph traversal of neighbors), the system first checks an **in-memory LRU cache** for the node’s block. If present and valid, it contains the latest version. If not in cache, the system queries the shadow delta store (`shadow.duckdb`). Because `__lmd_blocks` (the shadow table of blocks) contains the latest version of any recently modified block, a query like “SELECT blob FROM \_\_lmd\_blocks WHERE block\_id = ?” will return the newest data if that node was updated but not yet merged. Only if the node is absent in the shadow table does the system fall back to reading from the `graph.lmd` file on SSD. This lookup order (cache → shadow → base file) guarantees that the **newest committed version** of each node is read. It also means that recent updates (which reside in shadow) are accessible with low latency via DuckDB’s B-Tree index rather than scanning the large file.

**High-Level Write Path:** All index modifications use **copy-on-write** and indirection to preserve consistency. Rather than editing any data in place, inserting or updating a vector creates a new NodeBlock in memory (with a new version), leaving existing data untouched for concurrent readers. These new blocks are staged in the shadow store before eventually replacing the old ones in `graph.lmd`. This indirection (via shadow) allows **concurrent transactions** and readers to proceed without locking the large file; readers either get the old block (if they started before the change committed) or the new block from shadow (if they start after). Furthermore, the use of DuckDB’s transactional store for deltas ensures that partial failures (crashes in the middle of writes) do not expose torn or half-applied updates – the shadow table either has the new block or it doesn’t, thanks to atomic WAL commits.

## Issues in Prior Architecture and Shadow Design Mitigations

The shadow architecture was conceived in response to several shortcomings and failure modes identified in earlier designs. Key issues such as durability gaps, stale data reads, and complex index rebuild processes were raised in internal reviews. Below, we summarize each issue and describe how the new shadow system mitigates or resolves it:

### 1. Durability Gaps and Crash Recovery Inconsistencies

**Issue:** In prior approaches, updates to the ANN index were not guaranteed durable at transaction commit, leading to windows where a transaction could commit in the main database but the corresponding index update might be lost on crash. For example, a user inserts a new vector: the main DB commit succeeds (row is persistent, and mapping might be written), but the actual ANN node could be buffered in memory or a non-durable log. If a crash occurs just after commit, the index would start up missing that vector or with a broken mapping reference. This is a serious consistency problem: the base table and index could disagree on whether a vector exists. Another facet was handling **transaction aborts** – previously there was no robust mechanism to ensure that index changes from rolled-back transactions were discarded, potentially leaving *ghost nodes* that never got a matching commit.

**Shadow Mitigation:** The new design implements multiple strategies to eliminate these durability gaps:

* **Immediate Flush on Commit:** To narrow the commit-to-durability window as much as possible, the extension triggers an immediate flush of new node blocks to `shadow.duckdb` at the moment a transaction commits. Using DuckDB’s hooks (e.g., a `Transaction::RegisterCommitHook` or similar mechanism), the flush daemon is signaled to write out all dirty blocks from that transaction as soon as the main commit is done. This means that by the time the user sees a commit succeed, the index update is already in the WAL of `shadow.duckdb` (or will be within a few milliseconds), dramatically reducing the window where a crash can cause loss. The flush operation batches writes and uses `INSERT OR REPLACE` into the `__lmd_blocks` table to ensure idempotency (only the latest version of each block is kept). The WAL commit on the shadow DB makes the new blocks durable on disk, ensuring that **committed transactions have their index entries safely stored**.

* **Unified Commit Epoch & Visibility Filtering:** Every NodeBlock’s commit epoch ties it to a particular commit of the main database. On recovery, this helps reconcile state across components. For instance, if a crash occurred after the main DB commit but before the flush completed, the new row\_id->node\_id mapping might exist in `lookup.duckdb` but the NodeBlock wasn’t flushed. Upon restart, the commit epoch on that NodeBlock will be missing or set to an “invalid” marker (since it never made it to durable storage). The system can detect this and resolve the inconsistency. A likely strategy is: during index opening, scan the lookup table and ensure that for each mapping, either the node is present in `graph.lmd` or in the shadow table. If a mapping exists with no corresponding node data, the index can **tombstone or remove that mapping** (effectively dropping the index entry for safety, perhaps with a warning) or attempt a recovery rebuild for that vector. Because the mapping and data are always added together during normal operation, any discrepancy on recovery signals a crash mid-operation, which the system addresses by either re-applying the shadow log or cleaning up the orphan mapping. In practice, the immediate flush on commit and WAL replay cover most scenarios, making such orphaned mappings unlikely except in a narrow crash window.

* **Flush Barrier for Aborts:** To handle rolled-back transactions (so that no changes from an aborted transaction become durable), the flush daemon implements a **commit check** before writing any NodeBlock to `shadow.duckdb`. Each dirty entry carries the originating transaction’s ID, and the flush thread verifies the transaction status via DuckDB’s transaction manager API (if available) or an epoch system. Only entries from committed transactions are flushed; aborted ones are skipped. This ensures no “ghost” NodeBlocks from rolled-back transactions ever reach the disk. Additionally, the memory for uncommitted blocks can be allocated in a per-transaction arena that is freed on abort, so any in-memory state is also purged if a transaction fails. This two-layer approach (in-memory segregation and flush-time verification) closes the gap where previously an extension might not know about aborts.

* **Idempotent Merge and Recovery:** The merge (compaction) process – which moves data from `shadow.duckdb` into `graph.lmd` – is designed to be **crash-tolerant and idempotent**. If a crash occurs during merge (e.g., after some blocks have been written to the main file but before the shadow entries are cleared or meta updated), recovery will simply find that the shadow store still contains those entries (since the final step of clearing them didn’t commit). The merge can then be safely retried. Writing the same NodeBlock to the base file again is harmless because it’s the same data (or a no-op if it was already written). To support this, the merge procedure likely involves writing all pending blocks to `graph.lmd`, fsyncing, then within a transaction deleting them from `shadow.duckdb` and updating `graph.lmd.meta` (e.g., raising a merge sequence number) **after** the file is durably updated. Only when the transaction committing the deletions in `shadow.duckdb` succeeds do we consider the merge complete. If a crash interrupts the process at any point, either the shadow still has the blocks (so we can redo the writes), or if the shadow transaction committed but maybe a crash happened just after, the meta file’s version check will detect a mismatch and can trigger a cautious reload. The design even suggests using a version/merge sequence in both the meta and a manifest record to detect partial updates and avoid any confusion in recovery.

* **Atomic Metadata Updates:** The architecture avoids non-atomic file updates such as writing directly to `graph.lmd.meta` or renaming critical files without proper safeguards. As a mitigation for file-system level issues (especially on non-POSIX-compliant or network file systems where `rename()` might not be atomic), all critical file updates use a **write-temp-and-rename** protocol with fsyncs. For example, to update a “CURRENT” pointer or meta info, the system writes out a new file (e.g., `graph.lmd.meta.tmp`), fsyncs it, then renames it to replace the old meta, followed by a directory fsync. This ensures that even on crash or on file systems with weaker guarantees, we either have the old or new metadata intact, never a half-written file.

Together, these measures ensure **strong durability**: once a transaction commits, either its index updates will persist, or the system will detect and handle the discrepancy on recovery, never producing incorrect search results. The shadow WAL and commit protocol guarantee that **no partial index state is ever visible** to users: it’s all-or-nothing. This closes the durability gap present in earlier designs.

### 2. Stale Data Reads (Stale Neighbor or Vector Data)

**Issue:** In graph-based ANN, a common challenge for dynamic indexes is **stale neighbor data**. Each NodeBlock stores not just the node’s own vector, but also compressed copies of its neighbors’ vectors for fast distance calculations. If a vector **v** is updated (or a new vector is inserted), the neighbor blocks that contain v’s data may become stale. In the prior architecture (without a robust update strategy), this could mean that search traversals use outdated vector information, degrading accuracy over time. For example, if we change a vector’s embedding, or even insert a new node without updating neighbors, some blocks might still have old data until a full rebuild is done. The earlier design required either costly immediate updates to all affected blocks or risked returning slightly incorrect distances.

**Shadow Mitigation:** The shadow system addresses stale data through **eager update propagation for local neighbors and eventual consistency for wider changes**, combined with versioning:

* **Eager Neighbor Updates on Insert/Delete:** Whenever a new node is inserted, the algorithm selects some existing nodes to connect with (the new node’s neighbors). The design ensures that these neighbor blocks are immediately updated in memory to include the new node (or to remove a node on deletion) and marked dirty. Both the new node’s block and all *affected neighbor blocks* are flushed to the shadow delta store together at commit. This way, any search after the transaction will find that the neighbors’ blocks in shadow have been updated – they will include the new neighbor ID and the compressed vector for it. This **copy-on-write propagation** prevents stale reads in the most common case (topology changes due to inserts/deletes). The flush batching logic even deduplicates by block\_id, so if the same neighbor is updated multiple times in quick succession, only the latest version is kept. By always writing the latest version per block to shadow, we ensure queries never see an outdated neighbor list or missing link that should have been there.

* **Marking Version and Lazy Recompression:** Each NodeBlock carries a version counter or timestamp that is incremented on any change to its content (neighbors or itself). This version is stored alongside the block in shadow and base. If a vector’s own embedding is updated (a less common operation, effectively like a new insert for that row), the system can mark all of its neighbors as needing recomputation. The design could choose **not** to cascade the recomputation of compressed vectors immediately (to avoid excessive write amplification), but instead mark those neighbor blocks in metadata (or simply rely on the version timestamps). At query time, using the commit epoch and version, the search logic can decide to trust the stored neighbor distances or recompute on the fly if it detects a discrepancy. However, a more straightforward approach is taken during **merge**: when the background merge runs, it can **refresh stale neighbor vectors**. Since merge already reads each dirty block from shadow (which includes all recently changed nodes), it has an opportunity to recalc any derived data. For example, if node X’s vector was updated, when writing out node Y (a neighbor of X) the merge process can fetch X’s new vector and recompute the compressed form to store in Y’s block before writing it to `graph.lmd`. This ensures that by the time changes make it into the base file, all neighbor references are consistent and up-to-date. The review notes recommended clarifying this strategy, and the implementation will incorporate an **eventual consistency** approach: neighbor compressed vectors are updated *either at insert time or by the next merge*. There is thus a bounded window during which a neighbor’s cached vector might be slightly stale, but in practice, because we flush neighbors eagerly on structural changes, the only staleness can come from *value* changes of a vector.

* **Controlled Rebuild of Graph Structure:** The design acknowledges that maintaining graph optimality under many updates is complex. It incorporates **graph rebuild or optimization steps** in a controlled way. For instance, after a large number of insertions, a maintenance operation might recompute nearest neighbors for certain nodes (to ensure search quality). The **shadow architecture makes such rebuilds incremental**: one can insert updated neighbor lists into shadow as if they were normal updates, then merge. This avoids needing to rebuild the entire index from scratch. Essentially, even a “graph quality optimization” is just treated as another series of updates to NodeBlocks.

* **Testing for Staleness:** For safety, each neighbor entry in a NodeBlock could store a reference to the neighbor’s version or epoch. If during a search we access neighbor data and find a mismatch (neighbor’s current epoch != epoch of the cached vector), the system can detect it and potentially ignore the stale info. However, given the above approach, this may not be necessary unless live vector value updates are frequent.

**Trade-off:** The chosen mitigation strategy balances complexity and performance: it avoids a full cascade of updates on every vector change (which would be very expensive for high-degree nodes) by sometimes deferring the refresh to the merge or a maintenance phase. This eventual consistency is acceptable because the commit epoch mechanism will *never allow completely wrong results*: an entirely uncommitted or deleted node will be skipped. The only staleness is in distance computations, which might be slightly off until the next recompute. The design team deemed this acceptable given that ANN search is anyway approximate. Still, to minimize impact, critical changes (like neighbor list inclusion/exclusion) are done eagerly, and only the finer detail (vector compression updates) are deferred. Over time, periodic merges and possible **VACUUM/optimize operations** will clean up any remaining staleness.

### 3. Complexity of Index Rebuilds and Merges

**Issue:** Earlier designs for dynamic ANN indexing struggled with expensive index rebuilds and maintenance operations. If updates accumulated, one might have to completely rebuild the graph index (recompute all neighbor links) or write out a new copy of the index file, incurring huge I/O costs. A specific concern was internal fragmentation of the `graph.lmd` file over long-term use – deletions and updates could leave “holes” that either bloated the file or required complex compaction. The prior approach did not clearly define how to *incrementally* maintain the index structure, leading to either potential degradation or very costly periodic rebuilds.

**Shadow Mitigation:** The LM-DiskANN shadow architecture is explicitly designed to simplify maintenance by breaking it into **incremental, well-defined operations** rather than monolithic rebuilds:

* **Incremental Merge (Compaction):** Instead of rebuilding the whole index after a series of inserts/updates, the system continuously merges in changes from the shadow delta store. The merge operation can be tuned to run when the shadow log grows beyond a threshold (e.g., when `shadow.duckdb` has X MB of data or Y% of index nodes are updated). Because each merge only needs to process the blocks that changed since the last merge, its cost is proportional to the number of updates, not the total index size. For example, if only 5% of nodes changed, merge reads those from shadow and writes 5% of the blocks in `graph.lmd`. This is far cheaper than rewriting 100% of the index. The design even measured a prototype scenario: \~8GB index merging in 1 million updated blocks in \~38s on NVMe, showing scalability. The merge can also be parallelized or done in batches if needed (processing blocks in chunks, multiple threads writing different ranges, etc.).

* **Free List and In-Place Updates:** The `graph.lmd` file uses fixed-size slots for nodes, allowing **in-place overwrite** when merging. A node’s position in the file is determined by its node\_id (e.g., node\_id \* block\_size offset). This means we don’t have to relocate unchanged data during merge – we only overwrite the slots that have new versions. Deleted nodes simply free their slot (not reused immediately, to avoid node\_id reuse issues). A free list is kept in metadata for eventually reusing or compacting space. This approach eliminates the need to shift large swaths of data; merging an update is O(1) to seek and write one block. Fragmentation is handled by the free list and a possible **VACUUM INDEX** command that can compact space if needed. The vacuum process could, for instance, move the last block into a hole and truncate the file, updating the lookup table accordingly. However, to start, the simpler strategy is to only reclaim space at the end (tail trimming) and defer internal hole compaction, because internal moves require updating neighbor references if node\_ids change. The design suggests possibly avoiding node\_id reuse entirely to simplify this (deleted IDs remain tombstoned), and only vacuum out whole trailing ranges of blocks that are free.

* **Graph Quality Maintenance:** Full index rebuilds are largely avoided. Instead, **partial rebuilds** or **graph refinements** are done in place. For example, the insert algorithm uses a “prune” heuristic to maintain graph quality locally. Over time, if the graph’s quality degrades, one can run an offline process (or background thread) that takes batches of nodes and re-evaluates their nearest neighbors (this is akin to running additional insert operations in a batch). Those changes enter the shadow log as updates to neighbor lists. Thus, improving the graph is just more delta updates – applied transactionally and merged – not a full rebuild from scratch. The worst-case scenario (if the graph becomes very suboptimal or if major param changes) might involve rebuilding, but that can be done by bulk inserting all nodes into a fresh index directory. The normal path is to **never require a cold rebuild for routine operation**.

* **Mitigating Merge Bottlenecks:** For extremely large indices (billions of vectors), even merge could become heavy if not managed (since it touches many GB of data). The design provides options like **tiered merges** or limiting the scope of merges. For instance, it could merge in waves (merge 10% of shadow at a time) or maintain multiple delta levels (recent updates in memory, mid-term in shadow, older in base) to avoid any single huge compaction. Also, merge can be performed while queries run: since readers always check shadow first, they can continue to see updates that haven’t merged yet. During merge, to avoid conflicts, the process might acquire a short exclusive lock per block (to prevent flush thread from writing the same block). But it doesn’t need a full read lock on the whole index – queries can proceed, skipping blocks currently being written (or using the shadow version until the moment of switch). Proper locking (see Concurrency below) ensures no visible inconsistency. The result is that merges can happen **online** with minimal query downtime, unlike a full rebuild which would require reloading the index.

* **Simplified Backup/Restore and Rebuild:** Because each index is self-contained in one directory, operational tasks are simplified. Backing up an index is as easy as copying its directory (ensuring the copy happens after a merge or while flush is quiescent, or by also copying the shadow WAL file). In worst-case failure (e.g., both the `graph.lmd` and `shadow.duckdb` get corrupted beyond WAL recovery), one can reconstruct the index by scanning the base table: the extension could rebuild by inserting all vectors anew (effectively reindexing). This is obviously expensive, but it’s a last resort. More commonly, if just the lookup mapping is lost or inconsistent, it can be rebuilt by reading all NodeBlocks in `graph.lmd` (each NodeBlock can store its row\_id for this purpose). In summary, the shadow architecture turns rebuild into a manageable maintenance operation rather than a routine requirement.

### 4. Other Notable Issues and Mitigations (Concurrency, Consistency, Scale)

*(In addition to the three primary issues above, internal reviews highlighted concurrency complexity and certain edge-case conditions. We briefly note how the implementation handles these:)*

* **Concurrency & Deadlocks:** With multiple threads (user threads inserting/querying, background flush thread, background merge thread), careful locking is required. The implementation will use **fine-grained locks** on NodeBlocks (or shards of nodes) to avoid global locks. For example, each NodeBlock has a mutex; inserts lock the new node and its neighbors’ blocks while linking, flush locks blocks it’s writing, etc. A global `merge_mutex` might serialize the merge against other writers, but readers mostly use atomic epoch checks instead of heavy locks. The plan is to define a clear lock acquisition order (e.g., always lock lower node\_id first or use try-lock and back off to prevent cycles). Additionally, the extension initialization uses a global latch to avoid deadlocks when multiple indexes are attached simultaneously (ensuring one attaches fully before another begins). These measures mitigate the risk of deadlocks and race conditions in the complex multi-threaded environment.

* **Lookup Table Scaling:** The `lookup.duckdb` mapping table could grow very large (billions of rows for 10B-vector index). DuckDB’s indexing (B-tree on the PRIMARY KEY row\_id and perhaps a secondary index on node\_id) is expected to handle this scale, but performance will be monitored. If necessary, we could partition the mapping or use an alternative key-value store, but for now the simplicity of DuckDB’s single-file DB and its robust indexing is advantageous. Batch operations on the lookup (like bulk insert during index build) can be accelerated by using DuckDB’s append APIs with larger chunks.

* **Memory and Cache Management:** The shadow architecture offloads persistent storage to disk, but memory efficiency is still critical. The LRU cache for NodeBlocks will be tuned to only occupy a fraction of RAM (maybe caching the hottest 1-10% of vectors, depending on workload). If the cache is full, least-recently-used blocks are evicted – provided they are not dirty. Dirty blocks (unflushed updates) won’t be evicted until flushed. We also rely on the OS page cache for the `graph.lmd` file reads; large sequential reads (e.g., during merge or range scan) will populate the page cache which benefits later random reads. The extension avoids pinning huge amounts of memory via DuckDB’s buffer manager (an issue in a previous approach was that using DuckDB’s `FixedSizeAllocator` pinned all index blocks in memory) – instead, we manage reading/writing ourselves and only cache selectively. This gives us full control to ensure memory usage stays bounded no matter how large the index grows.

With these mitigations in place, the shadow architecture resolves the major pain points of earlier designs. **Durability gaps** are closed by WAL and flush-on-commit; **stale reads** are minimized by eager neighbor updates and MVCC rules; **rebuild complexity** is tamed by incremental merges and an update-friendly file format. We now turn to the detailed implementation plan reflecting this design.

## Implementation Plan and Technical Specification

In this section, we outline the concrete implementation approach for the shadow architecture in C++ (as a DuckDB index extension). This includes the key data structures, lifecycle of operations (insert, update, delete, flush, merge, recovery), integration points with DuckDB’s extension API, file format details, and concurrency control. The focus is on **how to build** the system described above, with guidance suitable for engineers implementing it.

### Data Structures and Components

#### Node Block and Related Structs (C++)

At the heart is the `NodeBlock` structure, representing a vector and its neighbors either in memory or as stored in the index file. Pseudocode for the struct might look like:

```cpp
struct NodeBlock {
    uint64_t node_id;          // Unique identifier (also determines file offset)
    row_t row_id;              // The DuckDB row_id this vector corresponds to (for optional redundancy)
    uint64_t version;          // Version counter for this block (increment on each update)
    transaction_t origin_txn;  // ID of creating transaction (for uncommitted blocks)
    uint64_t commit_epoch;     // Commit timestamp/epoch; 0 or MAX indicates not yet committed or deleted
    bool tombstone;            // True if this node is deleted (pending removal)
    VectorData vector;         // The raw vector data (e.g., float array of dimension D)
    std::vector<Neighbor> neighbors; // List of neighbor entries (each has neighbor node_id and compressed vector)
    // ... possibly checksums or padding to ensure fixed size ...
};
```

Each `Neighbor` entry would contain something like:

```cpp
struct Neighbor {
    uint64_t neighbor_id;
    CompressedVector comp_vector; // Compressed representation of neighbor's vector (e.g., PQ or ternary encoding)
};
```

The `CompressedVector` is specific to the compression used (ternary, PQ, etc.) and stores just enough info to compute distances quickly. The total size of a `NodeBlock` is fixed (e.g., 8192 bytes) so that `graph.lmd` can be indexed by block number.

We maintain a few key in-memory maps:

* **Mapping Table Cache:** While `lookup.duckdb` persists the row\_id -> node\_id mapping, the extension may also keep a hash map or ART (adaptive radix tree) in memory for quick lookups of node\_id by row\_id (to avoid a DB query each time). This in-memory map is populated at index load (scanning the lookup table or reading metadata). It must be kept in sync with inserts and deletes (updates).

* **Dirty Ring Buffer:** A lock-free (or mutex-protected) ring/circular buffer of dirty block entries to be flushed by the background thread. We define a `DirtyEntry` struct capturing a pointer or reference to a dirty NodeBlock, along with its block\_id (node\_id) and the transaction that dirtied it:

  ```cpp
  struct DirtyEntry {
      uint64_t block_id;
      NodeBlock *block_ptr;  // pointer to the in-memory NodeBlock (e.g., in cache)
      transaction_t origin_txn;
  };
  ```

  Writers will push `DirtyEntry` into this buffer whenever they modify a block (on commit). The flush thread will pop batches of these for writing to shadow.

* **LRU Cache:** Implemented perhaps with an LRU list + hash map from node\_id to NodeBlock\*. It stores NodeBlocks that are either clean (same as disk) or dirty (newer than disk, not yet merged). A dirty NodeBlock in cache must also exist in shadow store once flushed. We mark dirty vs clean for each.

* **Shadow DB Connections:** We maintain a persistent DuckDB `Connection` (or similar handle) to the `shadow.duckdb` database file, and another to `lookup.duckdb`. On index initialization, we `Connection::Open` both files (or attach them). For performance, we prepare commonly used statements:

  * `insert_stmt` for inserting or replacing into `__lmd_blocks` (shadow delta table).
  * `select_stmt` to fetch a block by block\_id from `__lmd_blocks`.
  * These can be `PreparedStatement` objects reused by flush and read operations to avoid SQL parse overhead.

* **Shadow Delta Table Schema:** Within `shadow.duckdb`, we create a table for blocks, e.g.:

  ```sql
  CREATE TABLE __lmd_blocks (
      block_id BIGINT PRIMARY KEY,
      block_data BLOB,       -- raw bytes of NodeBlock
      version BIGINT,
      checksum BIGINT        -- (optional) checksum for integrity
  );
  ```

  This schema allows fast PK lookups by block\_id. We may also use DuckDB’s WAL mode settings to ensure durability (DuckDB typically flushes WAL on each transaction commit; our flush thread groups many inserts into one transaction to amortize cost). Note: The `block_data` blob contains the serialized NodeBlock. We could choose to store separate columns for fields like commit\_epoch or tombstone for easier querying, but since reads/writes of shadow are always done via our code (we know the block\_id up front), keeping it as a blob is simplest.

* **Metadata Struct:** Representing the info in `graph.lmd.meta`:

  ```cpp
  struct IndexMetadata {
      uint64_t num_nodes;
      uint32_t block_size;
      uint32_t dimension;
      uint64_t merge_sequence;  // increments every time a merge (compaction) happens
      uint64_t free_list_head;  // maybe points to a linked list of freed node_ids (or 0 if none)
      // possibly, a checksum or magic number for sanity check
  };
  ```

  This struct can be read/written to the meta file with a simple binary write. The `merge_sequence` can detect if the meta was updated without corresponding data (version skew between meta and actual file or manifest).

#### Index Class and API Integration

We implement a DuckDB index extension, for example as `class LMDiskANNIndex : public duckdb::Index`. DuckDB’s `BoundIndex` interface requires us to implement methods for index operations. Key methods we’ll implement and their roles:

* **`Initialize(IndexInfo&)` or constructor:** Opens/creates the index directory and files. This will:

  * Create the directory if not exists, using DuckDB’s FileSystem API (to handle cross-platform paths).
  * Open or create `lookup.duckdb` and `shadow.duckdb` via DuckDB’s API (`DuckDB db(path)` and Connection).
  * If creating anew, execute the `CREATE TABLE lmd_lookup...` in lookup DB and `CREATE TABLE __lmd_blocks...` in shadow DB.
  * If opening existing, read the metadata file (`graph.lmd.meta`) and perhaps validate merge\_sequence vs a manifest (if any).
  * Load the mapping table (or at least initialize our in-memory row\_id->node\_id map).
  * Start the flush thread (see below).
  * Possibly schedule a merge if the shadow table is non-empty from last run (crash recovery scenario).

* **`Append(DataChunk &entries, Vector &row_identifiers)`:** Called when new vectors are inserted. For each new vector:

  * Allocate a new node\_id = `IndexMetadata.num_nodes + 1` (or from free list if reusing IDs). Then increment `num_nodes`.
  * Create a NodeBlock in memory for this vector. Compute its neighbors via the ANN graph insertion algorithm (likely a variant of DiskANN’s insert: perform a search to find nearest existing nodes, then prune neighbor list). This will involve reading some existing nodes – utilizing the cache/shadow/base layers as needed.
  * For each neighbor chosen, acquire that neighbor’s lock, update its neighbor list (add this node as neighbor, possibly evict one if max degree). Mark that neighbor’s NodeBlock dirty.
  * Fill this new NodeBlock’s neighbor list with the chosen neighbors and their compressed vectors.
  * Mark the new NodeBlock as dirty (it’s new, not on disk yet).
  * Insert the row\_id -> node\_id mapping into the lookup table *within the user’s transaction*. We can do this by executing a simple `INSERT INTO lmd_lookup VALUES (?, ?)` via the DuckDB API on the lookup connection. However, to keep it atomic with the user transaction, we might leverage DuckDB’s ability to piggyback on the transaction – possibly by attaching the lookup DB or by deferring commit (see **Transactional Coupling** below).
  * Append a `DirtyEntry` for the new block and each modified neighbor block to the ring buffer. We tag them with the current transaction’s ID (DuckDB provides the transaction context to the index during Append).
  * The actual persistence of these changes is deferred to either flush thread or at least until commit.

* **`Delete(DataChunk &row_identifiers)`:** Called when rows are deleted from the base table. For each row\_id:

  * Look up the corresponding node\_id (via our map or `lookup.duckdb`). If not found, nothing to do.
  * If found, mark that NodeBlock as tombstone: set `tombstone = true` and maybe set a deletion epoch.
  * Remove its entry from the lookup table (within the user transaction, via `DELETE FROM lmd_lookup WHERE row_id = ?`).
  * Update neighbors: for each neighbor of this node (we may need to load its block), remove references to the deleted node from their neighbor list. Mark those neighbors as dirty (so they get flushed).
  * Optionally, add the node’s ID to the free list (so its slot can eventually be reused or reclaimed by vacuum). We might decide not to reuse immediately to avoid reusing IDs within a running system (simpler for consistency).
  * Push dirty entries for all affected blocks as with insert.
  * The NodeBlock itself might remain in cache (marked deleted) until merge purges it. Readers will skip it because either the lookup mapping is gone (so they won’t ever traverse it) or the commit epoch indicates deletion.

* **Transactional Coupling:** A critical aspect is ensuring the mapping insertion/deletion is atomic with base table changes. **We will use DuckDB’s extension transaction hooks to coordinate this.** Since DuckDB does not allow writing to two attached databases in one SQL transaction, our approach is:

  * Perform the lookup table modifications using the *same connection* as the base (by attaching `lookup.duckdb` as an alias database to the main connection). E.g., `ATTACH DATABASE 'lookup.duckdb' AS idx_lookup;` then our index code can execute `INSERT INTO idx_lookup.lmd_lookup ...`. DuckDB ensures that if the main transaction aborts, changes to the attached DB are also aborted (it enforces single-db writes per transaction, so this might require the actual operation to occur on commit).
  * Another strategy is to defer the lookup DB commit until after the main commit, and in case of failure reconcile later. But to keep ACID properties, using `ATTACH` is cleaner if supported.
  * We will also use `Commit` and `Rollback` callbacks: DuckDB’s index API may allow a `Commit()` call after all operations of a transaction, where we can finalize things. If the main transaction aborts, our index should discard any buffered changes (the flush thread check covers this, but also we would not commit the lookup insertion). DuckDB might call `Rollback()` on the index, in which we can explicitly remove any partially applied mapping (in our case, if we inserted mapping eagerly, we’d need to delete it here). However, by using the attach method we let DuckDB handle it.

* **`Scan` (or `Search`)**: This index likely supports scanning for ANN queries (e.g., a table-valued function or a special predicate). If implemented, the `Scan` method would:

  * Take a query vector and optional filter, perform the DiskANN search (graph traversal). This means starting from some entry point nodes (maybe ones with highest degree or random sample, possibly stored in metadata), doing greedy search: for each current node, check its neighbors (fetch their NodeBlocks via our cache/shadow/base as needed), keep track of distance, etc., until convergence.
  * The result would be a set of candidate node\_ids (the nearest neighbors found). We then lookup their row\_ids via `lookup.duckdb` (either with one query using an `IN (...)` or multiple PK lookups).
  * The `Scan` outputs the row\_ids (and possibly distances if needed).
  * Concurrency: All reads use snapshot consistent with the transaction calling the scan. We ensure that while reading NodeBlocks, we skip any with commit\_epoch > current\_txn\_epoch (so uncommitted inserts from other txns are ignored). DuckDB’s index scan should already be running in a transaction context, so we have access to that.

* **`Merge` / `Vacuum` (Manual):** We may expose a method or command to force a merge (for admin or debugging). The actual merge will typically run in the background (flush thread or a separate compaction thread), but we might allow `VACUUM INDEX my_index` to trigger it. In DuckDB’s index API, there is a `Vacuum()` that can be implemented. Our `Vacuum()` could call `DoMerge()` internally.

* **`CommitDrop`**: If the index is dropped, DuckDB calls this to let us clean up. We should shut down flush thread, close files, and possibly delete the index directory.

#### Flush Thread (Asynchronous Delta Writing)

The flush daemon is critical for durability and performance. We implement it as a background thread that wakes up periodically or on signal:

* **Flush Trigger:** We use a condition variable or a lightweight notification mechanism. The thread sleeps for a short interval (e.g., 100–250ms) or until signaled (e.g., on transaction commit). On wake:

  * It checks the ring buffer for dirty entries. It will pop up to a certain batch size (say 1000 or 2048 blocks) to flush in one go.
  * It deduplicates the batch by block\_id, keeping only the latest DirtyEntry per node (since the same node might have been updated multiple times before flush).
  * It then opens a transaction on the `shadow.duckdb` connection (shadow DB allows only one writer at a time, which is fine). Batches inserts:

    ```cpp
    cx.BeginTransaction();
    for (auto &entry : batch) {
        insert_stmt.Bind(0, (int64_t)entry.block_id);
        insert_stmt.Bind(1, Value::BLOB(entry.block_ptr, BLOCK_SIZE));
        insert_stmt.Bind(2, (int64_t)entry.block_ptr->version);
        insert_stmt.Bind(3, (int64_t)ComputeChecksum(entry.block_ptr));
        insert_stmt.Execute();
    }
    cx.Commit();
    ```

    This pseudocode corresponds to using the prepared `insert_stmt` to put each block’s data into the `__lmd_blocks` table. We use `INSERT OR REPLACE` semantics for the statement so that if a block\_id already exists in shadow (an older version), it gets replaced by the new one.
  * The commit of this transaction writes the DuckDB WAL (grouping all inserts into one durable commit). We then mark those blocks in cache as *flushed*. “Flushed” means they are persisted in shadow. They may still be dirty relative to `graph.lmd`, but not lost on crash. In our implementation, we might clear a “dirty” flag or set a status to indicate “clean in shadow”. Readers can now load them from shadow if not in cache.
  * If the ring buffer was quite full, the flush thread may loop to flush the next batch immediately (avoiding producer outpacing consumer).

* **Coordination with Transactions:** As noted, on each user transaction commit, we will signal the flush thread (or even invoke it directly). We must ensure the flush happens **after** the mapping insertion transaction commits (so that if the flush writes out a new NodeBlock, the mapping for it is definitely present). Our commit hook ordering should ensure: main commit (row inserted, lookup updated) completes, then flush transaction begins. Since the main commit released locks, the flush can read the new mapping if needed (though it doesn’t need to). The key is durability ordering.

* **Thread Safety:** The flush thread will acquire locks on the NodeBlocks it writes (or we can design flush to operate on a snapshot of the block). A simple approach: require that any block in the ring buffer is effectively immutable (no further changes) until flushed. Writers either remove a block from cache or bump its version and place another dirty entry, but flush will always write a consistent snapshot of the block. The block’s data should not be freed while in ring; we keep it in cache until flush done. We can use a shared mutex: writers and readers lock shared for reading block content; flush locks exclusive to copy it for writing. Because the flush operation is relatively fast (a memory-to-DuckDB copy), contention is minimal.

* **Stopping the Thread:** On database shutdown or index drop, we signal the flush thread to exit (set `shutdown=true` and notify). It should flush any remaining entries and terminate. This ensures no dirty data is lost when closing gracefully.

#### Merge Process (Compaction to Base File)

The merge is typically initiated when the shadow delta store grows large. It can run in a dedicated thread or be triggered by a user call. Implementation steps:

1. **Prepare for Merge:** Acquire a global `merge_mutex` to prevent multiple merges or conflicts with flush:

   * Optionally, pause the flush thread or tell it not to start new flushes (we might let flush continue to work, but any new updates during merge mean those blocks will just remain in shadow – fine).
   * Ensure all current flushes are done (flush thread can be signaled to flush everything and wait).

2. **Gather Deltas:** Query the `shadow.duckdb` for all current entries in `__lmd_blocks`. We can do `SELECT block_id, block_data, version FROM __lmd_blocks ORDER BY block_id;`. Ordering by block\_id is useful for sequentially writing the base file. Because this can be huge, we might instead iterate with the DuckDB C++ API in chunks. Each row gives us a block\_id and the blob of data. Alternatively, since we have them in our cache and ring, we might iterate our in-memory structures. But reading from shadow ensures we capture any that might not be in memory (in case we evicted some dirty blocks after flushing).

3. **Write to graph.lmd:** For each entry from shadow:

   * Calculate file offset = block\_id \* BLOCK\_SIZE.
   * `pwrite()` the block\_data to the `graph.lmd` file at that offset.
   * It’s wise to do this in large sequential batches if possible: many block writes could be random, but sorting by block\_id makes it mostly sequential.
   * Use pread/pwrite with O\_DIRECT or normal writes with buffered I/O – depending on performance tuning. We likely use normal writes and rely on OS cache.
   * Keep track of any new end-of-file: if new nodes were added, `num_nodes` might increase, so we may extend the file.
   * After writing all blocks, call `fsync(graph.lmd)` to flush to disk.

4. **Update Metadata:** Update the in-memory IndexMetadata (e.g., set merge\_sequence += 1, set new num\_nodes, update free\_list if any changes). Also consider resetting the shadow DB’s state.

5. **Commit Changes in Shadow:** Now, within a single transaction on `shadow.duckdb`, delete all rows from `__lmd_blocks` (or drop and recreate the table for simplicity) and perhaps insert a marker that corresponds to the new merge\_sequence. We then commit that transaction. By doing deletions in the same atomic step, we ensure the shadow store is empty only if all writes succeeded.

   * If the system crashes *after* the file fsync but *before* this commit, it’s okay: on recovery, the shadow table still has entries (since the delete never committed), so the next open will simply attempt the merge again (and the blocks in `graph.lmd` are already up-to-date, but rewriting them is idempotent).
   * If it crashes *after* committing the deletion but maybe before meta file was written, then on reopen the meta vs actual file might show discrepancy. To handle this, we do the meta file update as part of durable steps: write new `graph.lmd.meta` (with updated merge\_sequence and counts) to a temp and rename it atomically. That way, either old meta is intact (then shadow still had data, which wouldn't happen if we committed deletion… so likely we update meta *before* deleting shadow entries? Actually, better: include the meta info in the shadow DB as well, or use a separate manifest file).
   * A robust approach: have a small file “CURRENT” that stores the last merge\_sequence and possibly a pointer (though here pointer is just an integer). We update it with atomic rename, separate from `graph.lmd.meta`. But it might be overkill. Simpler: because `lookup.duckdb` and `shadow.duckdb` are transactional, and `graph.lmd` is not, we rely on careful ordering and idempotency rather than cross-file atomicity.

6. **Post-Merge:** Release `merge_mutex`. Now the authoritative data is all in `graph.lmd`, and `shadow.duckdb` is empty. We can drop caches or mark all flushed blocks as clean (their content now matches base file). Queries will now get those from base (though our logic doesn’t need to change, since shadow has none, it falls through to base).

7. **Vacuum Consideration:** If there is a free list (holes from deletions), we might compact the file here. We could move the highest block into the freed slot and update lookup mappings for that moved node. However, this is complex in a graph (neighbor references by node\_id remain the same, so actually if we keep node\_id constant, moving blocks doesn’t change any references except the file layout). We would then have to update which node\_id is at end. To avoid this, initial implementation can simply not reuse node\_ids and not move anything, leaving fragmentation to be handled by an offline rebuild if needed. Or implement a conservative vacuum: if the last N blocks are free, truncate the file and adjust num\_nodes.

#### Crash Recovery Sequence

When the database (and extension) restarts after a crash or clean shutdown, the index should be made consistent:

* DuckDB will recover `lookup.duckdb` and `shadow.duckdb` automatically via their WALs (replaying any committed transactions).
* The index extension, on initialization, opens the meta file and both databases. It checks if the shadow table is non-empty:

  * If shadow has entries, it implies the last merge did not clear them (either due to crash or because merges are pending). In this case, those entries represent updates not reflected in `graph.lmd`. We must ensure they are applied or at least used. Two choices: **Option A:** immediately rerun the merge process to apply them to `graph.lmd` (catch-up merge on startup). **Option B:** simply continue operating with those deltas in place (i.e., treat it as if the system never went down – queries will find latest data in shadow anyway). Option B is simpler: we don’t necessarily need to merge on every restart, we can just carry on with a larger shadow. However, if the crash happened mid-merge, some blocks in `graph.lmd` might already be updated. That’s fine because shadow still has them too, and our read logic prioritizes shadow, effectively ignoring base’s partially updated state. We can schedule a merge later to clean it up.
  * If shadow is empty, we trust `graph.lmd` to be the full state.
* The mapping table `lookup.duckdb` is opened. We might verify its consistency with the number of nodes. For debugging, we could scan `graph.lmd` and count nodes or verify a sample to ensure mappings are correct. In normal operation this shouldn’t be an issue, but if any row in lookup points to a node beyond `num_nodes` or that is tombstoned with no data, we log a warning and optionally remove it.
* Resume the flush thread, etc.

Edge case: if the crash occurred right after a commit that added a mapping but before flush of the NodeBlock (the earlier discussed window), on recovery:

* The new mapping is in lookup (DuckDB committed it) but the NodeBlock was not in shadow (not flushed) and obviously not in base. Our read logic would, on a query for that row\_id, find the node\_id from lookup, then check cache (empty), shadow (miss), base (some old or empty block). It could either find nothing (if base file not extended) or stale data (if node\_id was reused – which we try to avoid).
* We mitigate this by not reusing node\_ids and by perhaps storing an “expected node count” in the meta. If lookup has a node\_id that is >= meta.num\_nodes but that block is not in shadow, we conclude it was a crash-lost update. We can choose to drop that lookup entry (the index entry is lost, unfortunately) or attempt recovery by reinserting the vector. Since reinserting requires the original vector data, which we can get from the base table (the row still exists with its vector). A possible automated recovery: call the ANN insert procedure for that row\_id again during startup. This would regenerate the NodeBlock and neighbors. This is feasible since we can query the base table for that vector. Implementing this adds complexity, so initially we might document that in a crash in that tiny window, the index might require a manual reindex of that row (or user re-insert). However, the flush-on-commit greatly minimizes the chance of this happening.

#### DuckDB Extension Hooks and Lifecycle

We leverage DuckDB’s extension API at several points:

* **Index Registration:** During extension load, we register `LMDiskANNIndex` as a new index type, likely via `ExtensionUtil::RegisterIndex(...)`. This associates our index with a SQL keyword or so (e.g., `USING DISKANN` in a `CREATE INDEX` statement).

* **Create Index Call:** When the user creates an index, DuckDB will call into our code (perhaps a static Create function) with index parameters (like dimension, metric, etc.). We use those from `IndexInfo` or parse from the `WITH (...)` clause (like in `config.hpp` we have parameters for block\_size, etc. that come from SQL options). We then call our `Initialize` logic as described.

* **Serialization:** DuckDB may ask the index to serialize its state into the main DB (so that the index persists when the main DB is closed). However, because our index has external storage, we might not need to serialize the whole structure. Instead, we can store in DuckDB only a reference, e.g., the path to the index directory or a unique index ID. Possibly the `IndexInfo` in main DB contains just a token that maps to our on-disk files. DuckDB’s `Serialize` method could write such metadata into the main DB. On database reopen, `Deserialize` would then instantiate our index, pointing it to the directory. (Alternatively, because our index is created via SQL DDL referencing the table, we may not need DuckDB to persist it – we will recreate/open it on each attach explicitly.)

* **Commit and Rollback Hooks:** If available, these are crucial to coordinate with transactions. Lacking a direct callback for rollback in DuckDB (not exposed to extensions easily in older API), we rely on our flush barrier technique and the main DB’s behavior: since we only commit shadow changes after commit, aborted txns simply never flush and their mapping insert (if done via attach) is auto-aborted by DuckDB. If we do any memory cleanup on rollback, we may implement `Index::Rollback()` if provided.

* **Drop:** On `DROP INDEX`, we get `CommitDrop` call. We should safely shut down threads and close DB connections. Also delete files if appropriate (DuckDB usually expects that dropping an index frees resources; deleting files is safe because the user explicitly dropped it). Since it’s folder-per-index, a simple FileSystem::RemoveDirectory on the index folder can wipe it, but only after closing the DB files.

* **Threading and Task Scheduler:** We use C++11 threads for flush and perhaps merge. Optionally, we could integrate with DuckDB’s TaskScheduler (schedule tasks that run on DuckDB’s worker threads). The design notes that using DuckDB’s scheduler is a way to avoid managing our own threads. For simplicity, one thread per index for flush is okay. For merge, we might spawn a thread on demand or reuse flush thread occasionally.

### Concurrency and Edge Case Handling

The implementation must handle several edge cases and concurrency scenarios gracefully:

#### Concurrent Readers vs. Writers

Readers (ANN queries) can run concurrently with writers (inserts/updates) and the flush/merge threads. We ensure safety via:

* **Copy-On-Write NodeBlock updates:** Writers never modify a NodeBlock that readers might be accessing; instead, they make a new copy or work on a private version. If an existing node’s neighbor list is changed, we either copy that node’s block to a new buffer (update version) or at least lock it during modification. Because queries only need read locks, they won’t conflict unless they happen to be reading exactly when we lock. Using try-lock with short hold times (just to copy neighbors) means minimal wait.

* **Block-Level Locks:** A fine-grained mutex for each NodeBlock (or a group of them) is used. When updating neighbors, we lock each involved block in a globally consistent order (e.g., always lock the smaller node\_id first to avoid deadlock). Readers can either not lock at all (if using atomic epoch checks) or take a shared lock. Likely easier: readers don’t lock; they read possibly stale data but then use epoch check to validate. However, since we copy on write, a reader could either see old block (before update) or new block after it’s inserted in cache. It won’t see a half-done update because the update happens under lock and then replaces the old entry in cache.

* **Ring Buffer Coordination:** Writers pushing to the dirty ring should ideally not block. If the ring is full (flush is behind), the writer might have to wait or allocate a bigger buffer. We size it generously (e.g., can hold many thousands of entries). Alternatively, drop flush signals more frequently if it gets near full.

* **Flush vs. Read:** When flush thread writes a block to shadow, that block might be being read by a query. Since flush doesn’t remove it from cache or change it, and since DuckDB’s MVCC means the query’s snapshot would either see the old or new depending on timing, we handle it as: if a query started before the writing transaction committed, it should ignore the new block anyway (commit epoch check). If it started after, it may either get the data from cache (which flush thread will not remove) or if cache was cold, it might query shadow. If the flush transaction hasn’t committed yet, a concurrent query that misses cache and checks shadow may not find the new block (since flush not committed), but then it will read the base file. That’s okay because that query’s snapshot might logically be just before the commit. In other words, the timing aligns with transactional semantics: a query either sees all of a committed transaction’s effects or none, never partially. Our design enforces this by tying the flush to commit.

* **Flush vs. Merge:** We avoid them stepping on each other. A simple rule: do not run flush and merge concurrently. Either pause flush during merge or lock out merge until flush empties the queue. This prevents a scenario where merge writes some blocks while flush writes others, which could intermix sources of truth. Instead, before merge, flush everything and quiesce (and perhaps temporarily stop accepting new updates by quiescing transactions or diverting them to a new shadow log – that gets complex, so better to coordinate merges during low activity or via explicit admin command).

#### Crash During File Operations

We already covered the strategy for atomic file updates. To reinforce:

* When writing `graph.lmd.meta`, always use atomic replace via temporary file and `fsync`.
* If using a manifest file (like a “CURRENT” pointer to a specific meta or data file version), use a robust protocol (write new, fsync, rename).
* If `graph.lmd` itself gets corrupted or partially written (unlikely with careful fsync ordering), on open we can detect via checksum mismatches or by DuckDB not opening (for meta). We then could restore from shadow or backup.
* After crash, the idempotent merge logic means we might rewrite some blocks twice but end up consistent.

#### Partial Index Rebuild / Recovery

If something truly goes wrong (e.g., shadow and base diverge in an unexpected way, or the user aborted a long merge leaving many updates unmerged), the system may perform a targeted rebuild:

* If only lookup table is lost: regenerate from blocks as noted.
* If a few NodeBlocks are missing (due to the rare mapping-without-block scenario), reinsert those from base table.
* If the entire graph needs rebuilding (due to structural degradation), consider an offline tool to rebuild by reading all vectors and inserting into a new index.

These are last-resort measures. The normal recovery should rely on WAL and merge idempotency to self-heal without manual intervention.

#### Lookup Inconsistency Handling

We take care that the row\_id → node\_id mapping stays consistent:

* On insert, if the same row\_id already exists in mapping, we throw a constraint error (DuckDB’s transaction will abort). This ensures no duplicates.
* On delete, if the mapping is missing, that’s fine (means index entry wasn’t present).
* If a user performs a DuckDB **ROLLBACK**, DuckDB will remove any inserted row from the main table and also from our attached lookup table automatically. Our flush thread will see that the transaction aborted (no commit epoch) and not flush the NodeBlock, effectively dropping it. Thus, no mapping should exist without a node, and no node without mapping, after any clean rollback.
* In case of a crash inconsistency (mapping without node or vice versa), the open procedure or a special consistency check can reconcile:

  * **Dangling mapping (no NodeBlock):** As discussed, possibly re-index that row or drop the mapping.
  * **Dangling node (NodeBlock in shadow/base with no mapping):** This could happen if the lookup transaction committed but main rolled back? But our approach doesn’t allow that (they’re in one txn). Or if someone manually tampered. In any case, a node with no mapping is effectively unreachable in queries (since queries start from row or from entire graph – but if a node has neighbors it could still be traversed! That’s dangerous). To handle, each NodeBlock could also store its row\_id; on open we could scan all NodeBlocks in shadow and base to ensure their row\_id exists in the base table. If not, that node is a ghost and should be removed. This scan is expensive at scale, so we prefer to rely on transactional guarantees to prevent this scenario in the first place.

### Example Lifecycle Flows

Bringing it all together, here are the step-by-step flows for typical operations, integrating all components:

#### Insert Transaction Flow

1. **Begin Transaction** (DuckDB main engine).
2. **User INSERTs** a new row with a vector into the base table.
3. **Index::Append** is called for that row:

   * Acquire necessary locks for new node and candidate neighbors.
   * Compute `node_id = next_id` (say it was 1005).
   * Create NodeBlock for node 1005 with the new vector.
   * Perform neighbor search: find nearest M existing nodes (using current index data structures).
   * Update node 1005’s neighbor list and for each neighbor node (say nodes 7, 42, 88), update their neighbor list to include 1005.
   * Mark NodeBlock 1005 and NodeBlocks 7,42,88 as dirty in cache.
   * Push DirtyEntry(1005), (7), (42), (88) into ring buffer.
   * Execute `INSERT INTO idx_lookup.lmd_lookup (row_id,node_id) VALUES (new_row_id,1005)` via attached lookup connection.
4. **User commit** transaction:

   * DuckDB commits the main table insert and the attached lookup insert. This assigns a commit id (epoch) E.
   * Our commit hook triggers: signal flush thread to process the ring buffer for txn entries.
   * Assign commit\_epoch = E to NodeBlock 1005 (and any updated neighbor blocks, though they were existing so they already had a commit\_epoch from when they were created; updating their neighbors doesn’t change their own commit epoch, but we might update a “last\_updated” field).
   * Flush thread wakes (or was already waiting on condition):

     * It pops DirtyEntries for 7, 42, 88, 1005.
     * Deduplicates (maybe 1005 is last, neighbors earlier).
     * Begins a DuckDB txn on shadow\.duckdb; does INSERT OR REPLACE for each (7,42,88 might replace their old entries if they had one, 1005 is new).
     * Commits. WAL fsync occurs – now blocks 7,42,88,1005 are durable in shadow.
   * The flush can optionally mark these blocks as “clean in shadow” (or remove from ring).
5. **Post-commit**: The base table now has the row, lookup has the mapping, shadow has the block data. Any new transaction’s query will see commit\_epoch E as <= its snapshot, so it will include node 1005 in searches.

#### Query (Search) Flow (after above insert)

1. User queries the ANN index (e.g., a UDF or `ANN_SEARCH(table, query_vector, k)`).
2. The extension’s `Scan` is called with the query vector:

   * It picks initial node(s) (from a start list, or a random sample of node\_ids).
   * Suppose it starts with node 42. It calls `GetNode(42)`:

     * Cache miss (if not in memory or outdated).
     * Queries shadow: `SELECT block_data FROM __lmd_blocks WHERE block_id=42`. Since we updated 42 in the insert, shadow\.duckdb **has** an entry for 42 (with the neighbor list including 1005).
     * It retrieves the blob, deserializes to a NodeBlock (possibly also inserts into cache for reuse).
   * Now it has Node 42 with neighbors including 1005. It computes distances from query to all neighbors (using neighbor comp\_vectors stored).
   * It moves to next node in search (maybe 1005 is now a candidate). It calls `GetNode(1005)`:

     * Cache miss, check shadow: finds 1005’s block (since it was just inserted and not merged yet).
     * Deserializes Node 1005 (which has neighbors 7, 42, 88).
   * Continues graph traversal until k nearest are found.
   * The resulting node\_ids (including possibly 1005) are then converted to row\_ids by a lookup query: e.g., `SELECT row_id FROM idx_lookup.lmd_lookup WHERE node_id IN (...)`. Because we inserted the mapping, node\_id 1005 returns the correct row\_id.
3. The query returns the final row\_ids to the user. The user is oblivious to the fact that some were served from the shadow store; it’s seamless.

#### Delete Transaction Flow

1. User deletes a row (with row\_id, say 42) from the base table.
2. Index::Delete is called with row\_id 42:

   * Look up mapping: row\_id 42 -> node\_id N (suppose N=42 as well for simplicity, or any number).
   * Lock NodeBlock 42 and its neighbors.
   * Mark NodeBlock 42 as tombstone (and perhaps set origin\_txn to current txn, commit\_epoch to 0 again to indicate pending deletion).
   * For each neighbor of 42 (say 7, 88, 1005), remove 42 from their neighbor list, mark them dirty.
   * Remove mapping via `DELETE FROM idx_lookup.lmd_lookup WHERE row_id=42` (DuckDB will ensure this is done in txn).
   * Push DirtyEntries for neighbors (7,88,1005).
   * (Optionally, add 42 to free\_list for later reuse or just keep tombstoned.)
3. User commit transaction:

   * DuckDB commits main deletion and lookup deletion.
   * Commit hook: flush thread signaled.
   * Assign commit\_epoch = E to all affected neighbor blocks (they get a new version indicating they dropped 42 at epoch E). For node 42, since it's deleted, we can either leave commit\_epoch as the original insert epoch but tombstone=true, or set a special “deleted” epoch (like commit\_epoch = E and tombstone flag on).
   * Flush thread flushes neighbors 7,88,1005 to shadow (replacing their old entries). We may or may not flush the deleted node 42’s block; it might not be necessary to flush a tombstone since lookup is gone and no query will ask for 42 by ID. However, if node 42 might still be reached via graph traversal (some other node still has it until we removed all references), after neighbor updates it should no longer appear in any neighbor list. So node 42 is effectively isolated. We could flush its block as a tombstone record in shadow so that if someone does try to get it (e.g., during merge or by a direct search starting at 42 if 42 was an entry point), we know it’s deleted. Flushing it might be good for completeness.
   * WAL written, all changes durable.
4. Post-commit:

   * Base table row gone, lookup entry gone. If a query now tries to search, it will not encounter node 42 in any neighbor list (since all those were updated). If it somehow did (maybe an outdated cache?), the commit\_epoch or tombstone flag would cause the search to skip it.
   * Eventually, merge will reclaim node 42’s slot in file (or leave a hole).

#### Index Merge (Compaction) Flow

1. Condition: shadow\.duckdb has grown (e.g., 100K entries). The system decides to merge.
2. Either automatically via flush thread or on a `VACUUM INDEX` call, `DoMerge()` is invoked.
3. Acquire `merge_mutex`.
4. Pause flush thread (or drain remaining entries).
5. Read all entries from shadow\.duckdb (`SELECT block_id, block_data, version FROM __lmd_blocks`).
6. For each entry:

   * Write to file at block\_id \* BLOCK\_SIZE.
   * If block is tombstoned (deleted), we might simply skip writing or write a known “deleted” pattern. Actually, better to remove it: we could also maintain in meta a list of deleted IDs.
   * Recompute any necessary derived data (e.g., if we deferred neighbor vector updates, do them now before writing).
7. Fsync the file.
8. Write updated meta (with new node\_count if increased, etc.) to `graph.lmd.meta.tmp`, fsync, rename.
9. In shadow\.duckdb, either `DELETE FROM __lmd_blocks` or drop table and recreate (which also clears it). Commit that.
10. Release merge lock.
11. Resume flush thread (if paused).
12. Now shadow is empty. All dirty blocks now have their changes reflected in base file. We mark all NodeBlocks in cache as clean (their `version` on disk equals in memory). We can also reset commit\_epoch of those blocks if needed (not really, commit\_epoch stays for MVCC).
13. The index is compacted; future queries will find nothing in shadow, only base.

#### Concurrent Queries During Merge (if allowed)

If we permit queries to run during merge, we must ensure:

* If a query comes for a node that’s currently being written, one approach: leave the shadow entry until after merge commit, so the query will actually still find it in shadow (since we didn’t delete yet). Or simpler, hold a read lock on the index during merge to block queries briefly. But for large merges, blocking queries is undesirable.
* Because our read logic prefers shadow, and we haven’t yet deleted from shadow, queries will continue to see the old path. Only once merge commits and shadow is cleared do new queries switch to base. This implies there is a moment when a query right after merge might still be looking at outdated cache while new data is in base; but since we updated cache or cleared it, they should get base. This gets tricky but can be managed by invalidating caches at the right time.
* We likely choose to quiesce queries for the small window of finalizing merge (maybe the system can tolerate that as merges are infrequent and possibly done in off hours).

### Conclusion

This implementation plan translates the LM-DiskANN shadow architecture into a concrete design. We have described how the **shadow delta database** and **lookup table** are used to ensure durability and transactional consistency, and detailed how each operation is handled. The solution addresses prior shortcomings by **never updating in place**, carefully coordinating commits across the base table and index, and using standard database techniques (WAL, MVCC, manifest/metadata atomicy) to guarantee correctness.

With this design, we expect to handle indexes on the order of 100M to 10B vectors on SSD with high throughput. Reads are optimized by the block caching and on-disk layout, and writes (updates) are efficient due to batching and sequential file access. The most complex parts – like concurrency and crash recovery – are mitigated by the robust shadow/indirection approach, as detailed in the above specification. Each potential issue raised in design reviews has a corresponding mitigation here, making the architecture resilient and practical for implementation.

**Sources:**

* LM-DiskANN Shadow Table Design Proposal
* Design Review & Risk Analysis
* Mitigation Matrix for Failure Cases
