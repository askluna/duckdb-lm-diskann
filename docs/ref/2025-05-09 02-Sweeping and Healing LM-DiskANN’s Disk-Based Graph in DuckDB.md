Great. I’ll prepare a design study focusing on sweeping and healing the LM-DiskANN disk-resident graph in `graph.lmd`, with particular emphasis on supporting backlinks within a fixed node block size and ensuring zero-copy, SIMD-optimized access patterns.

I’ll analyze node-local backlink layouts vs external representations, tradeoffs between SMID-optimality and stability during tombstoning, and detail the full sweeper lifecycle — including edge healing, tombstone reclamation, block rewrite strategies, and crash safety. I’ll also integrate guidance for zero-copy serialization (e.g. via `cista`) and compressive neighbor quantization.

I’ll let you know when the full design is ready.

# Design Study: Sweeping and Healing LM-DiskANN’s Disk-Based Graph in DuckDB

## Introduction and Goals

LM-DiskANN is a disk-native graph-based ANN index that stores each vector and its neighbors in fixed-size blocks (e.g. ~4KB pages). It supports dynamic updates (insertions and deletions) by maintaining *complete routing information* per node – i.e. each node’s block contains its ID, full vector, and a list of neighbor IDs along with compressed neighbor vectors (using ternary quantization). This design minimizes RAM usage (no global in-memory vector store) at the cost of extra disk space. Our goal is to design a **sweeping and healing** mechanism for this on-disk graph (`graph.lmd`) within DuckDB. This includes: (1) robust **backlink storage** strategies, (2) a background **sweeper lifecycle** for tombstone cleanup and graph maintenance, (3) an **edge-healing algorithm** to repair graph connectivity after deletions, (4) a **tombstone reclamation policy** for space management, (5) analysis of **performance/stability trade-offs**, and (6) ensuring a **zero-copy, SIMD-friendly** storage layout. We assume the index’s parameters (vector dimensionality and max neighbors *R*) are fixed at creation, and neighbors’ vectors use ternary quantization (e.g. 2 bits/dimension) for compact storage. The design must integrate with DuckDB’s MVCC (multi-version concurrency control) and WAL (write-ahead logging) to preserve ACID properties, while maximizing search recall and minimizing write amplification over potentially millions of deletions.

## 1. Backlink Storage Layout Options

Efficient **backlink storage** (i.e. storing reverse edges or incoming neighbor references) is critical for identifying and healing broken connections after deletions. We compare three approaches to maintaining backlinks in the LM-DiskANN index, focusing on their impact on write amplification, block utilization, ease of tombstoning, and healing accuracy:

- **(A) Node-Local Inline Backlinks:** Store incoming neighbor references *within each node’s block*, alongside the outbound neighbor list. In practice, this means each edge is stored twice (once in each endpoint’s neighbor list), maintaining full symmetry at all times. This is essentially the current LM-DiskANN approach: when a node is inserted, it is added to each of its neighbors’ lists and vice versa. This yields a fully symmetric k-NN graph on disk. The layout remains fixed-size (padding unused slots with zeros up to R), and can be made SIMD-friendly by aligning arrays within the 4KB block (e.g. aligning the start of the compressed neighbor vectors to 32-byte boundaries for efficient `Highway` loads). The duplication of edges increases space overhead (the index may be ~50–300% larger than storing each edge once), but it enables **zero-copy** neighbor access: the search algorithm can retrieve a node’s entire neighbor list from one disk read, with no indirection. Write amplification is higher, since each insertion or deletion updates multiple blocks (all affected neighbors). However, updates are node-local writes of ~4KB each, which leverage sequential access patterns if neighbors’ blocks are nearby on disk. Tombstone handling is straightforward: a deleted node *and* its ID in other nodes’ lists can be marked as tombstones in-place. Healing is also simplest in this model – since all neighbors of a removed node know about each other via the deleted node, we can directly connect them (ensuring minimal loss in graph connectivity, as discussed in Section 3). In summary, approach (A) maximizes read locality and healing completeness at the cost of extra write I/O and storage redundancy.
- **(B) Centralized Reverse Index:** Maintain a **global reverse index** mapping each node ID to the list of nodes that have it as a neighbor. This could be stored as a separate structure (e.g. a disk-resident table or a memory-mapped array of pointers into neighbor lists). In this design, each node’s block stores only its outbound neighbors (like a directed graph), and incoming links are tracked centrally. One implementation is a fixed-size array of length *N* (number of nodes), where each entry contains pointers or offsets to a list of inbound neighbor IDs (possibly stored in a contiguous chunk or a log). This reduces duplication of neighbor IDs on disk – each edge appears only once in a node’s forward list – cutting storage overhead roughly in half. **Write amplification** for insertions can improve: adding a new node requires writing its block and appending its ID to the *reverse lists* of each of its neighbors (which could be batched in one area). Notably, we can defer or batch updates to neighbor blocks: the new node’s neighbors might *not* immediately add the node to their outbound list. Instead, the reverse index remembers that “neighbor X should eventually include new node Y”. This laziness can save up to *R* block writes on insertion, at the cost of a temporary asymmetry in the graph. During search, if the graph is treated as undirected, we would consult the reverse index to discover incoming connections. (For example, if node X is being expanded, we check if any new node Y has X in its reverse-list and include Y as a neighbor candidate.) This ensures new nodes are reachable even before all outbound lists are updated. **Read cost:** A central index lookup is an extra step – however, it can be optimized. The reverse index, if kept in memory or in a memory-mappable structure, supports fast lookup of a node’s incoming neighbors with one additional I/O (which may be sequential if the reverse lists are stored contiguously). The overall search may slightly slow down (due to checking both out-links and in-links), but this impact is bounded if most edges are eventually made symmetric. **Tombstoning:** Deleting a node in this scheme means marking its block as deleted and removing its ID from the central reverse index entries of all neighbors. The removal in the central index could be done by marking tombstones in those lists or by lazy accumulation (similarly to insertions). This central mapping makes it easy to find *all* references to a deleted node without scanning the entire graph, simplifying correct healing. **Healing accuracy:** If not all edges are immediately symmetric, the graph might momentarily lose some connectivity until healing runs. However, because the reverse index captures every lost connection, the healing process can use it to restore symmetry: for each neighbor that lost an inbound link, we know exactly which node was lost and can seek alternate connections. Thus, eventual healing precision is on par with approach (A). In summary, approach (B) trades off some immediate consistency for lower write amplification – inserts and deletes mostly append to or mark entries in the reverse index (sequential writes), with neighbor blocks updated in batch by the sweeper. The complexity of maintaining a separate index and consulting it during searches is the main drawback.
- **(C) Log-Structured Backlink Storage:** Use a **log-structured or delta-based** approach to record neighbor changes, deferring in-place updates to node blocks. In this design, neither neighbors’ blocks nor a central index are immediately updated on insertion/deletion. Instead, we append operations to a *neighbor delta log* – for example, “Node P deleted” or “Node X should add neighbor Y”. The actual node blocks are updated by a background merge process that ingests these log records. This approach is inspired by LSM-tree and systems like FreshDiskANN, which buffer updates to avoid random writes. **Write amplification** can be the lowest: each insertion/deletion initially writes only a sequential log record (and perhaps marks the affected node’s own block), avoiding the *R* random writes altogether in the short term. Periodically, a compaction routine (the sweeper, see Section 2) applies the log entries: building small *patches* of the graph and writing them back to the main index in batches. For example, if node P is deleted, we log this event, and later the sweeper will remove P’s entry from neighbors’ lists and connect those neighbors, all in one batch write per page. **Block size utilization** remains high because we always rewrite full blocks, possibly merging several small changes at once. **Read cost:** During normal search, we may need to consider the unmerged log: e.g. if a node is deleted but its entry still exists in neighbor lists on disk, the search should ignore it (requiring a check against a tombstone list or delta log in memory). Similarly, if a new edge is not yet applied to a node’s block, the search might consult the log to see if an inbound connection exists. This adds some overhead, but this can be mitigated by applying deltas frequently or by using an *online merge*: the query engine skips tombstoned IDs and can treat “newly added neighbor” log entries as additional edges. **Healing precision:** Immediately after a deletion, the graph may be in a suboptimal state (neighbors of P not yet connected). However, because the deletion and potential new connections are recorded, the eventual healing (when the log is merged) can be as accurate as the other approaches. There is a *window of reduced recall* until the sweeper runs – we might quantify this: for example, if the sweeper lags and 1M deletions accumulate, the search recall could degrade (as found in prior work where graph quality degrades with many unhealed deletions). The key is that this approach **decouples write cost from immediate consistency**: it favors throughput (especially bulk inserts/deletes) and defers accuracy restoration to background processes. Crash resilience is maintained by logging every change in the WAL and the delta log; on recovery, we can replay the log to catch up outstanding changes.

The table below summarizes these formats:

| **Backlink Storage Format**       | **Write Amplification**                                      | **Read Cost**                                                | **Healing Precision**                                        |
| --------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Node-Local Inline (Symmetric)** | **High:** every update writes ~R+1 blocks (duplicate edges)  | **Low:** one random read yields all neighbors (no indirection) | **High:** immediate symmetry; minimal connectivity loss upon deletion |
| **Central Reverse Index**         | **Medium:** update new node’s block + append ~R entries in central list (deferred neighbor block updates) | **Medium:** 1 extra lookup for in-neighbors during search/healing | **High:** eventual symmetry; uses reverse list to fully restore links (minor transient loss) |
| **Log-Structured (Delta Log)**    | **Low:** sequential log appends for updates; batch rewrite later | **Medium:** must check log for tombstones or pending edges on reads | **High (eventual):** initial connectivity loss until background patching; fully healed after merge |

**Note:** All approaches ultimately aim to preserve an *undirected* graph for search. The differences lie in when and how the symmetry is achieved (eagerly vs lazily). In our DuckDB integration, approach (A) provides simplicity and query-time performance, while (B) and (C) improve update throughput. A hybrid scheme is also possible (e.g. maintain a lightweight reverse index in memory even if storing edges inline on disk, to accelerate sweeper operations).

## 2. Background Sweeper Lifecycle

We propose a dedicated **background sweeper** to manage tombstones and maintain graph integrity without impeding foreground queries. The sweeper’s responsibilities include: removing or rewriting blocks with deleted nodes, healing broken edges, and garbage-collecting freed space. It operates in *epochs* coordinated with DuckDB’s MVCC to ensure consistent views. Key design elements of the sweeper system:

- **Tombstone Queue & Trigger:** When a node is deleted (via a SQL DELETE or similar API), we mark its entry as a *tombstone* (e.g. a flag in its block header or an entry in a small “deleted nodes” table). This marking is a light-weight, transactional update logged in the WAL (to guarantee durability). The deleted node’s ID is then enqueued for the sweeper. We ensure this initial deletion is efficient – it does not immediately purge or restructure the graph, it simply *labels* the node as deleted and (optionally) records the set of its neighbors from its block. This design yields low p99 delete latency; as observed in LM-DiskANN, deletion is fast because it “only needs to stitch the hole made from the removed node” (no expensive searches). In our approach, even the stitching (healing) can be deferred to background, making the user-facing delete nearly O(1). The sweeper can be triggered by various conditions: a threshold on number of tombstones, a timer (periodic maintenance), or when the system is idle.

- **Epoch Coordination (MVCC):** DuckDB uses MVCC epochs to isolate transactions. We leverage this to ensure **crash safety and reader safety** during sweeping. Each deletion occurs at a specific epoch. The sweeper will not physically reclaim a tombstoned node’s block until no active transaction can still “see” it. In practice, we can store the deletion epoch with the tombstone and consult DuckDB’s global minimum active epoch. This is similar to how DuckDB defers reclaiming deleted tuples until an old snapshot is gone. By doing so, long-running readers that started before the deletion will continue to see the node’s data (if they happen to traverse it) until they finish, after which the node can be removed. This yields **consistent reads** without locking the whole index. For coordination, the sweeper thread may periodically grab a latch or check a shared atomic value to get the latest safe reclamation epoch.

- **Safe Removal and Copy-on-Write:** To physically remove a tombstoned node, the sweeper follows a **copy-on-write (CoW) rewrite flow**:

  1. It locates the node’s block (by ID -> block address mapping, since node IDs are likely their index in the file) and reads its neighbor list (which may be cached from the deletion moment).
  2. It then initiates *healing* for those neighbors (see Section 3) – this may involve updating each neighbor’s list (adding new links, removing the deleted ID).
  3. Rather than modifying neighbor blocks in place (which might be concurrently read by queries), the sweeper creates **new versions** of those blocks. For each neighbor that needs changes, we allocate a new 4KB block (from a free list or at file end, see Section 4), copy the neighbor’s data, apply the edits (removing tombstoned references, inserting any new links), and write this new block to disk. We also prepare to update the index’s metadata so that references to that neighbor now point to the new block (this might be as simple as an atomic pointer swap in an in-memory block table, or marking the old block as obsolete and the new block as current in a small index header).
  4. All these changes (identifiers of rewritten blocks, updated neighbor lists, freed block IDs) are recorded in the WAL before being made durable. Once the new blocks are flushed, the sweeper *installs* them, e.g. by updating a mapping from node ID -> block address to point to the new locations. This can be done under a lock or by an atomic pointer update (since each node’s block pointer can be treated like a versioned object).
  5. Finally, the old blocks (the deleted node’s block and any neighbor blocks that were rewritten) are marked as invalid/free. Importantly, we do not free them to the OS or reuse them for new inserts until it’s safe (ensuring no active query is still pinning the old block in memory).

  This CoW approach, akin to an LSM tree compaction or a shadow paging, ensures that readers either see the old version of a neighbor (before healing) or the new one, but never a half-updated partially healed state. It also integrates with DuckDB’s WAL and recovery: in case of a crash after writing new blocks but before fully updating metadata, the WAL can replay the intended updates or roll back to the pre-sweeper state. Page-level recovery is preserved because we always write new pages and then atomically swap references (no torn writes to existing pages).

- **SSD-Aware Page Selection:** The sweeper should choose *which pages/blocks to clean or compact* based on fragmentation and write amplification considerations. For example, if using approach (C) (log-structured deltas), it might choose a set of cold pages with many tombstones to rewrite in one batch (similar to a segment compaction in log-structured storage). If using approach (A) with immediate updates, the need for page rewrite is lower, but eventually, many deletions could leave “holes” (tombstone entries) in neighbor lists that waste space or affect search performance. The sweeper can maintain statistics: e.g., count of tombstones per block, or a fragmentation ratio (unused bytes in the 4KB vs. used). When a block exceeds a threshold (say >20% of neighbor slots are tombstones), it becomes a candidate for rewrite. By scheduling larger batches of page writes, the sweeper can amortize overhead and also be kind to the SSD (avoiding frequent small random writes). It may even sort candidates by disk location to group writes (reducing seek overhead). This process is analogous to vacuuming pages in a database or cleaning segments in an SSD flash translation layer.

- **Queuing and Throttling:** We envision the sweeper running continuously at a low priority, or in bursts. A queue of *sweeper tasks* is built: each task might be “process deleted node P” or “compact page X”. We use a priority system: e.g., deletion healing tasks have higher priority than pure compaction. The tasks can be processed one by one, but to avoid interference with foreground I/O, we might throttle the sweeper (e.g. process at most N pages or M bytes/sec, or only run during off-peak times). DuckDB being an in-process DB, we ensure the sweeper yields to the query threads and possibly integrates with DuckDB’s scheduler. One could use condition variables or `std::jthread` with a stop token to manage the background thread’s lifecycle (stopping it on database shutdown, etc.).

**Lifecycle Summary:** In steady state, the background sweeper continuously monitors the index. When nodes are tombstoned, it collects them, waits for safe epochs, then rewrites affected blocks and heals edges. It also reclaims space to a free list and keeps the on-disk graph healthy (no excessive accumulation of deleted entries). All actions are logged for crash recovery. This design ensures *delete operations have minimal latency* (just marking tombstones), while heavy work is done asynchronously. The result is a stable index size and search performance even under a stream of updates.

## 3. Edge Healing Algorithm

When a node is removed, its absence can “break” edges in the graph – both outgoing links (from the deleted node to others) and incoming links (others pointing to the deleted node) are lost. The **healing algorithm** must restore connectivity and preserve the quality of approximate search. We base our approach on the LM-DiskANN deletion strategy, with enhancements inspired by graph repair algorithms like Vamana and possibly DASR (a diversified graph pruning technique). The goals are to use *backlink-derived in-degree info* to identify where edges are missing, find good replacement neighbors, and ensure forward ↔ reverse link symmetry post-healing.

**Key idea:** When node **P** is deleted, its neighbors should ideally be connected directly to each other to “stitch” the hole P leaves. If the graph was fully symmetric, P’s neighbors set N_out(P) (also N_in(P)) contains all nodes that were connected via P. After deletion, each node in N_out(P) may have lost one connection (to P), reducing their degree. To heal, we make each such neighbor **n** “meet” P’s other neighbors. Specifically, for each neighbor **n** ∈ N_out(P), we propose to add all other former neighbors of P (i.e. N_out(P) ∖ {n}) to **n**’s neighbor candidates. This is exactly what LM-DiskANN’s LM-Delete does: *“for each neighbor n of p: N_out(n) ← N_out(n) ∪ (N_out(p){n}), then prune.”* We adopt this as the starting point. However, we must handle it carefully in a disk-based setting and possibly limit the fan-in to avoid overload. Pseudocode for healing one deletion is given below:

```pseudo
Function HealDeletion(deleted_node P):
    neighbors = N_out(P)  // list of P's neighbors (from P's block)
    For each node n in neighbors:
        oldList = N_out(n)
        // 1. Add all other neighbors of P as candidates to n
        newCandidates = (neighbors ∖ {n}) 
        N_out(n) = oldList ∪ newCandidates   // union (deduplicated)
        // 2. Prune n's neighbor list to enforce max degree R and graph quality
        distances = [] 
        For each m in N_out(n):
            dist = Distance( vector(n), vector(m) )
            distances.append((m, dist))
        // Sort candidates by distance from n (using compressed vectors for efficiency)
        sort(distances, by=dist)
        // Apply pruning heuristic (Vamana or DASR):
        pruned = []
        For each (m, dist_nm) in distances:
            keep = true
            For each x in pruned:
                if dist(x, n) * α ≤ dist(n, m): 
                    // m is too close to an existing neighbor x (within α factor)
                    keep = false 
                    break
            if keep:
                pruned.append(m)
            if pruned.size == R:
                break
        N_out(n) = pruned
        // (Symmetry fix will be handled after loop)
    EndFor
    // 3. Restore symmetry: ensure each new connection is bidirectional
    For each node n in neighbors:
       For each m in N_out(n):
           if n ∉ N_out(m):
               N_out(m) = N_out(m) ∪ {n}
               // (Optionally prune N_out(m) if it exceeds R)
```

This procedure does the following:

- **Candidate generation:** Neighbors of P are introduced to each other. If node *n* had neighbors (A, B, P, C) and P’s set was {n, X, Y}, after deletion we add X, Y as potential neighbors of *n*. This may temporarily increase *n*’s degree beyond R.
- **Distance Computation:** We compute distances from *n* to all these candidates. Thanks to storing compressed neighbor vectors in each node’s block, we can compute distances on-the-fly with SIMD (e.g. using Highway vectors to compute dot products or L2 distances between the quantized embeddings) without extra disk I/O. This distance computation can be done entirely in-memory if the neighbor blocks are cached/pinned by the sweeper.
- **Pruning (Vamana-style or DASR):** We then prune the list back to size R using a heuristic. The pseudocode above demonstrates a common strategy used in DiskANN/Vamana: iteratively select the closest neighbor and remove any other candidate that is too close to a selected neighbor (within a threshold α). This maintains the “Generalized Sparse Neighborhood Graph (GSNG) property”, which balances graph sparsity and search connectivity. We could also incorporate a **DASR (Diversified Adaptive Selection and Rewiring)** criterion if available – for instance, emphasizing diversity in angles or ensuring some minimum in-degree. In practice, Vamana’s approach is effective: it retains the most relevant neighbors and drops redundant ones.
- **Symmetry enforcement:** After pruning each neighbor’s list, we have to ensure the new edges are reciprocal. The above loop adds edges to N_out(n) but we must also add *n* to each of those neighbors’ lists. Many of those edges will already exist (e.g. if we connect X and Y through P, when we handle X’s turn, we add Y, and when handling Y, we add X). But to be safe, a final pass ensures that for every edge (n, m) we added, we also have (m, n). This may temporarily violate the R limit on *m*’s side, so we may prune *m* as well (in practice, this can be folded into the pruning step: we could perform pairwise healing – connecting all P’s neighbors in a clique-like fashion, then globally prune each affected node). The result is a symmetric set of links among the former neighbors of P.

The **healing budget** per deleted node primarily involves computing distances and pruning for the neighbors of P. If P had degree *d_P* (<= R), we are adding at most *d_P-1* new candidates to each of those *d_P* neighbors. In worst case, d_P = R (e.g. 70). So we might add up to 69 candidates to each of 70 neighbors, ~4830 pairwise distance computations. Each distance is in a D-dimensional space (say D=128) and can be computed via dot product or L2 using SIMD. Using 128-D and 2-bit quantized vectors, we can process maybe 16 dimensions per SIMD register; rough estimate: 128 dims / (16 dims per 256-bit register) = 8 operations per distance. 4830 distances * 8 = ~38,640 SIMD ops. Even accounting for some overhead, this is quite manageable in a background thread (and much less than inserting a new node which requires a full graph search). In practice, *d_P* will vary – many nodes have smaller degree than R, especially after prior prunings, so the typical healing cost is lower. We can further throttle healing work by limiting the number of new connections (the parameter **γ**, the *replacement fan-in*). For example, we might decide not to connect *all* neighbors of P, but only the top-γ closest among them for each neighbor. A smaller γ reduces the candidate count (and write amplification) but might leave the graph slightly less connected. There is a trade-off: a higher γ (approaching R) yields more robust healing (higher recall post-deletion) at cost of more computation and possibly more neighbors being added (which might temporarily violate degree limits until pruned). We could choose γ based on desired recall – e.g., if we allow γ = R-1 (fully connecting P’s neighbors) we maximize connectivity as LM-DiskANN does. If we choose γ = say 10, each neighbor only gains up to 10 new edges, containing the explosion of edges in very high-degree deletions.

**Backlink usage:** If we have a centralized reverse index (approach B) or log, the healing algorithm would retrieve not just P’s explicit neighbors but also any node that considered P a neighbor (if asymmetry existed). Those are essentially the inbound neighbors. We would unify those sets and treat them all as neighbors to inter-connect. This ensures no component of the graph is stranded. The algorithm above assumed symmetry (N_out = N_in for P), but if not, we augment the `neighbors` list with any inbound-only neighbors as well. The rest of the process remains similar: connect them all, prune.

After healing, the graph should maintain **forward↔reverse link symmetry**. Every edge lost by deletion is compensated by new edges that are present in both endpoint lists. The sweeper will write the updated neighbor lists as described in Section 2. By doing this in the background, we hide the latency from the user. Empirical results from LM-DiskANN show that such local healing yields minimal impact on recall – deletion latency was uniform and graph quality remained high after many deletions. In our design, if 1M random deletions occur, the healing will incrementally ensure the search recall remains near the original. We might expect a slight drop in recall if the sweeper falls behind, but once all healings are applied, the recall should recover to the level of an equivalent index built without those 1M points. This stability is a key advantage of aggressively healing (as opposed to simply “lazy deletion” where many missing edges accumulate and degrade search quality).

## 4. Tombstone Reclamation Policy

Efficient space reclamation is essential to keep the `graph.lmd` file size stable and reduce I/O. We define a **tombstone reclamation policy** that decides when to rewrite or free pages containing deleted entries, how to reuse freed space, and how to do so safely in DuckDB’s environment with MVCC and buffer management.

**Page Rewrite and Fragmentation Thresholds:** Each node’s block has a fixed capacity (1 ID, R neighbor IDs, R compressed vectors). When some neighbors are deleted, their IDs in other lists become tombstones (invalid entries). If a node accumulates too many tombstones in its neighbor list, the effective degree shrinks and the block has “holes.” We set a threshold – e.g. if > X% of the entries in a block are tombstones (or simply if > Y tombstone entries) – then that block is a candidate for recompaction. The sweeper will copy the block to a new location, omitting tombstones (and optionally padding the end with zeros again). This not only frees space in that block for future neighbors (though in a fixed-size block model it just reduces fragmentation) but also reduces scanning overhead (search algorithms won't waste time on as many tombstoned entries). Another trigger is if an entire node is tombstoned (obviously that block can be freed). Finally, if the *free space ratio* in the whole file grows large (many deleted nodes not yet freed due to safety epochs), we might proactively reclaim to keep the file size bounded.

**Free-List Design and Reuse:** We maintain a **free list** of 4KB blocks that are available for allocation. When a node is deleted and it’s safe to drop its block, we add that block’s offset to the free list. Similarly, when the sweeper rewrites a block to a new location, the old block becomes free. The free list could be a simple vector of free block IDs in memory, periodically synced to disk (perhaps in the index header or a separate free space table). Allocation strategy could be LIFO (reuse the most recently freed block to keep cache locality) or some smarter strategy like segregating by zones (to reduce fragmentation of the file into holes). Given DuckDB’s append-only storage model (it often doesn’t shrink files until a checkpoint/vacuum), our free list will mostly serve to reuse holes for new insertions, rather than actually truncate the file frequently. Over time, if many deletions occur and free list grows, new insertions of vectors can fill those slots. We ensure that a freed block is only reused after all transactions referencing the old data are gone (thanks to epoch checks as above). In terms of implementation, accessing and modifying the free list should be thread-safe – e.g. protected by a mutex or atomic operations, since the main thread inserting new nodes and the sweeper freeing nodes might concurrently update it. A lock-free structure could be used, but given the relatively low rate of index updates compared to queries, a simple `std::mutex` guarded list is fine.

**Generational Compaction:** One challenge with a long-running index is **free space fragmentation** – scattered free blocks could hurt sequential I/O. We might employ a generational or tiered compaction: for example, the index file could be conceptually divided into segments; if a segment has many free blocks, we can *compact that entire segment* by moving the still-live blocks to a new area and then freeing the whole segment (making future allocations there contiguous). This is analogous to generational garbage collection or leveled compaction in LSM trees. Recently deleted nodes (younger generation) might be handled with minor compactions (small patches), whereas if over months lots of deletions accumulate, a major compaction can defragment. This policy can be tied to DuckDB’s `CHECKPOINT`: perhaps on a checkpoint event, we also do an index compaction, since checkpoints already create a consistent point to potentially rebuild or reorganize on-disk structures.

**Buffer Pool Integration (Pin/Unpin):** DuckDB operates with a buffer manager that pins pages in memory when accessed. Our sweeper must cooperate with this. For example, if a query is currently scanning node block X (pinned in memory), and the sweeper wants to rewrite or free block X, it should detect that and either wait or create a new version and mark the old as to-be-freed later. We can utilize DuckDB’s existing pin/unpin hooks: before rewriting a block, attempt to pin it exclusively or mark it as evictable. If it’s in use (pinned by an active query), we defer until that query unpins it. This is usually short – queries in ANN search will pin a node only to examine neighbors and then unpin quickly. Alternatively, since we do copy-on-write, we can actually create the new block without touching the old. Once the new block is ready, we can atomically swap a pointer so that new queries pin the new block. The old block might still be pinned by a query that started earlier; we rely on the epoch mechanism to not free it yet. This interplay ensures we never invalidate a pointer that a query thread holds. DuckDB’s design where you need `CHECKPOINT` to truly drop free space may mean that our free blocks remain in the file. That’s fine – the free list ensures they get reused. If the user does request a `VACUUM`-like operation on the index, we could then compact and truncate any trailing free blocks.

**Long-Lived Reader Safety:** As alluded, we won’t reuse or release a block ID if doing so could break a reader. By tagging each free block with the epoch it was freed in, we can refrain from reusing it until the global last reader epoch surpasses that. This is akin to epoch-based reclamation or a generation count on the free entries. In practice, unless someone holds a transaction open for a very long time, this delay is small. If there is an analytic query that open a transaction and scans millions of vectors (perhaps reading the index in parallel), our sweeper will still function but just hold off freeing certain blocks. Even in that scenario, since deletions are presumably transactional, a query either sees a node or not; there’s no half measure.

**Preventing Snowball Effect:** One subtle policy point – when we heal edges by adding connections, some nodes’ neighbor lists might grow and require pruning. Pruning can remove some existing neighbors (to respect R). If those removed neighbors drop below a certain in-degree, would we consider that a “damage” to heal? We have to be careful not to get into an endless loop of healing-prune-healing. The GSNG property ensures that the graph remains navigable even if not fully complete graph, so we accept the pruned outcome as final. We do not attempt to heal the removal of an edge due to pruning – that is intentional. Only removal due to node deletion (tombstone) triggers healing.

In summary, our reclamation policy **reuses space aggressively** to keep the index size bounded (important for SSD longevity and performance), but **never compromises active queries or consistency**. We leverage DuckDB’s transactional infrastructure (epochs, WAL, buffer manager) to implement a safe garbage collection of graph nodes. Over a long period of insertions and deletions, the file should neither bloat nor require full rebuilds – space from deletions is either reused or can be compacted out during maintenance windows. This is a major improvement over naive approaches that would leave “ghost nodes” or require manual index rebuilds to reclaim space.

## 5. Performance and Stability Trade-Offs

Every design choice above affects performance (throughput, latency) and index stability (recall, disk wear, etc.). We analyze these trade-offs:

- **Deletion Throughput vs Search Throughput:** By handling heavy deletion work asynchronously, we decouple their performance. A user delete transaction only does a quick tombstone mark (often a millisecond or less), and the sweeper handles the rest. This means even if a flood of deletions comes (e.g. deleting 1M vectors one by one), the database can acknowledge them quickly, and the background thread will queue up the work. The risk is if deletions come faster than the sweeper can process, the backlog grows. In such a scenario, search queries might start seeing degraded performance or recall (because many tombstones are unhealed). However, our design tries to mitigate this: search will still skip over deleted nodes (they’re marked, so they won’t be considered final results, though they might be visited during search unless filtered out early). The recall impact of unhealed deletions was a concern raised in literature; FreshDiskANN demonstrated stability by background merging to prevent degradation. Our sweeper similarly ensures that even large deletion batches eventually result in a fully healed graph. As long as the average delete rate doesn’t perpetually exceed the sweeper’s capacity, the system will catch up and maintain high recall. Empirically, LM-DiskANN’s deletion method showed only slight latency increase on a denser graph and presumably negligible recall loss after healing. With our more flexible design, we could see near-original recall even after 1M random deletions, as new shortcuts will connect what those 1M nodes used to connect.
- **I/O Amplification and Tail Latency:** By performing batch page writes, we improve overall I/O throughput at the expense of *some* additional I/O volume. For example, node-local updates (A) write immediately but in small chunks; log-structured (C) writes larger sequential batches. On SSDs, sequential writes are very fast, and wearing is less of an issue if the total bytes written is similar. However, approach (A) could have more random write patterns (many scattered 4KB writes). Approach (C) will have bursts of sequential writes (which SSDs handle well) followed by large reads if we compact segments. The tail latency (p99) for individual operations is crucial:
  - For **deletions**: p99 delete latency in our design remains low, since we don’t wait for edge healing or disk compaction in the user thread.
  - For **queries**: The p99 query might be slightly impacted during an ongoing sweep if, say, the sweeper is rewriting a page that a query needs (the query might wait briefly to pin it or get a cache miss). But because our operations are copy-on-write, the old page is still available to serve queries while the new is being written. At worst, a query might encounter more tombstoned entries (requiring skipping) if healing hasn’t caught up, which could make that query traverse a bit more of the graph. Still, since R is bounded (neighbors lists are at most R, e.g. 70), skipping a few tombstones doesn’t blow up search complexity significantly. Thus, p99 query latency might see a small constant factor increase in a heavily churned index, but not an unbounded degradation.
  - **Concurrency**: multiple threads inserting or querying concurrently with the sweeper can cause cache thrashing or lock contention. We minimize contention by design: the sweeper mostly works on separate data (old blocks being freed, new blocks not yet visible to queries). We use fine-grained locks or atomic swaps for integrating new blocks. So the impact on throughput is low.
- **SSD Write Amplification:** Frequent rewriting of blocks can cause additional writes beyond the logical amount of data. We aim to minimize this. Node-local approach inherently writes twice as many edges (so roughly 2x logical writes vs a minimal directed graph). But that might be acceptable given the read efficiency gained. The background compactions (in approach C) consolidate multiple changes, which is efficient, but they do rewrite some data that hasn’t changed (neighbors that remain). In worst case, if every node eventually gets rewritten due to neighbors deleting, we might rewrite the whole index multiple times over its life. Still, this is similar to defragmenting storage; many databases rewrite pages on checkpoints, etc. We would quantify this by, say, simulating a workload of 1M deletions on a 50M vector dataset and measuring total bytes written. Likely, healing each deletion touches O(R) neighbors. If each neighbor’s block is 4KB, that’s up to R*4KB written per deletion in worst case (e.g. 70*4KB = 280KB per deletion). 1M deletions -> 280GB written if worst-case every deletion rewrote 70 neighbors. In practice it will be much less, as neighbors cluster and multiple tombstones can be cleaned in one rewrite. Also, if the same page accumulates multiple tombstones, we rewrite it once for many deletions. FreshDiskANN’s results or similar systems could be cited to show that maintenance writes are an order-of-magnitude smaller than rebuilding from scratch constantly. We also consider SSD longevity: the design should avoid pathological cases of rewriting the same blocks repeatedly in a short span. Our generational approach helps (don’t rewrite the same page too often; maybe impose a minimum interval between rewrites of the same page to let more changes accumulate).
- **File Size Stability:** Our free list reuse policy ensures the file doesn’t grow indefinitely with deletions and insertions. Without it, every insert after deletes might append new blocks, leading to file bloat. With reuse, the file size will plateau around the high-water mark of number of nodes. DuckDB’s checkpointing could be leveraged to actually shrink the file if needed by writing a compact copy of the index and swapping files (this could be an offline operation, since truncating in place is tricky under concurrency). However, routine operation shouldn’t require that; the index file will have a stable size except for incremental growth when new nodes exceed previously freed space. We ensure that even after 1M deletions, the file has 1M free slots which will be consumed by future inserts or remain as overhead at most. If the application simply deletes without inserting, one might want to shrink the file – we could provide a command to rebuild the index from live data if needed, or just accept the sparse file (the space can be reclaimed by OS if the file is sparse).
- **Recall and Search Accuracy:** The ultimate measure for an ANN index is how well it recalls true nearest neighbors after a series of updates. A poorly healed graph might lose recall (some vectors become unreachable or distances in graph increase). Our healing algorithm explicitly attempts to preserve connectivity. By linking all former neighbors of a deleted node, we effectively ensure that any two nodes that were connected via P now have at most one extra hop path (they can reach each other through one of the other neighbors). In graph terms, if P was a bridge between two clusters, removing P and connecting its clusters together ensures the graph remains connected. If P was part of a tightly knit cluster, those links were probably redundant anyway, so removal is fine. We also maintain the *routing* property by reusing the same pruning logic as used in index construction, so the graph structure post-healing satisfies the same properties (GSNG) as an index built from scratch. Therefore we expect **no significant recall drop** even in large deletion scenarios. In fact, experiments in LM-DiskANN implicitly show that after deletion and stitching, the search performance remains on the same recall-vs-latency curve. If anything, a very slight increase in graph degree in local areas could even improve recall (at cost of a bit more traversal) – but we cap degrees to R to avoid blowing up query time.

In conclusion, our design strives for a **balance**: background work amortizes expensive operations, preserving smooth online performance (low tail latency for queries and deletes), while aggressive healing and compaction preserve the *quality and size* of the index over time. The chosen policies (like when to sweep or how many edges to add) can be tuned to the workload: a high-update-rate environment might choose approach (C) with low γ to keep write amp minimal, accepting a tiny recall hit until sweeper catches up; a mostly-static environment can afford approach (A) for absolute best query speed and immediate consistency. DuckDB’s transactional framework helps us achieve these without risking corruption or inconsistent reads.

## 6. Zero-Copy Serialization and SIMD-Friendly Layout

To maximize performance, all on-disk structures are designed for **zero-copy** access and efficient SIMD processing. This means a node’s data can be read directly from the memory-mapped file or buffer into the index’s data structures without transformation, and algorithms can utilize wide vector instructions on these data.

**NodeBlock Structure:** Each node is stored in a binary layout that mirrors a C++ struct. From LM-DiskANN we have: `{ uint32_t id; float vector[D]; uint32_t neighbor_ids[R]; uint8_t comp_vectors[R][Q]; }` (where Q = bytes per compressed vector, e.g. 32 bytes for ternary-quantized 128-dim). This structure is padded to 4KB. We ensure proper alignment of fields: for instance, align the `vector` on a 16 or 32-byte boundary (since it’s 512 bytes, it will naturally align to 4 anyway, but aligning to 32 helps SIMD loads). The neighbor_id array is 4-byte aligned, and the comp_vectors start on a 4-byte boundary; we could add an explicit padding after neighbor IDs to align the comp_vectors on a 32-byte boundary (e.g., in the earlier calculation, neighbor IDs took 280 bytes which left comp_vectors misaligned by 4 bytes; adding 4 bytes padding would align them). This way, loading a neighbor’s compressed vector into an XMM/YMM register can be done with an aligned load (`HWY_ALIGN` macros from Highway or simply ensuring our pointer is aligned). The consequence is that we can call `Distance()` in the pseudocode above on an array of bytes representing a vector and have it vectorized.

**Zero-Copy via Cista:** We plan to use a serialization library like **Cista** (or similar) to define our on-disk structures. Cista allows defining a struct and directly memory-mapping a file into that struct representation (provided we avoid non-POD pointers and use offsets instead). Our NodeBlock fits this: it’s a fixed-size POD (plain data) struct of only primitive types and arrays. We avoid any internal pointers; references between nodes are by ID, not by memory pointer. This means we can mmapp the entire `graph.lmd` file into memory (read-only or read-write as needed) and treat it as an array `NodeBlock nodes[num_nodes]`. Accessing node i’s neighbors is then just `nodes[i].neighbor_ids` – no deserialization step, no memory allocation. This is truly zero-copy deserialization. If the file is too large to mmap entirely on 32-bit systems, we’d rely on the buffer manager to load pages on demand, but even then we copy bytes from disk into the same struct layout in memory.

**Bulk Read/Write and Mmap:** The structures are chosen so that bulk I/O can be done by reading consecutive blocks. For instance, when building the index or during recovery, reading the graph.lmd file sequentially will pull in nodes in order. Because each NodeBlock is self-contained, we could even support memory-mapped *read-only* usage for querying – the OS page cache will bring in pages as queries access new nodes. Write-ahead log is used to make modifications atomic, so the memory map can be refreshed or invalidated after recovery. If not using mmap, the DuckDB buffer manager can still read 4KB pages and we can reinterpret_cast the page bytes to a NodeBlock struct. This is safe due to alignment and fixed layout.

**SIMD Processing of Vectors:** Both distance computations and scanning neighbor lists for a specific ID can benefit from SIMD:

- *Distance computations:* Because we store both the query vector (in memory, uncompressed perhaps) and neighbor vectors (compressed in node blocks), we can vectorize the distance calculation. For example, if using Hamming distance or dot product on 2-bit quantized values, we can pack 16 or 32 such values in a 64- or 128-bit vector and use lookup or XOR operations to compute partial distances. The **Highway** library provides high-level abstractions to do these in a portable way (e.g., loading 16 bytes at a time from two arrays and computing difference). Ensuring 32-byte alignment on these arrays lets us use efficient loads, avoiding split load penalties (which, while smaller on modern CPUs, still use extra bandwidth).
- *Neighbor list scanning:* For operations like finding a tombstone ID in a neighbor list or checking membership, SIMD can compare multiple IDs at once. For example, to remove P’s ID from neighbor n’s list, we can load 8 or 16 IDs at a time in a SIMD register and compare to P (with a single instruction like PCMPEQD on x86 to get a mask of matches). Highway or `<std::simd>` in C++ could help implement this succinctly. This zero-copy layout (an array of `uint32_t neighbor_ids`) is ideal for such vectorized scanning. If we had a more complex structure (e.g. list of pointers), we couldn’t do this easily.

**WAL and Endianness:** Because we are mapping raw structures, we must account for endianness if portability is needed (DuckDB is little-endian on all common platforms, so we can define the on-disk format as little-endian and use appropriate conversions if ever read on a big-endian machine). Cista can generate portable representations if needed. In our context, we likely don’t need cross-platform portability of the index file, but it’s something to note.

**Future Dimension or R changes:** We assumed fixed D and R for simplicity. If in future we want to allow a different dimensionality or neighbor count (say user chooses R=100 for a new index, or uses 256-dim embeddings), our structures should accommodate that without code redesign. We can include metadata in the index file header indicating D, R, and compression scheme. Our NodeBlock struct could be templatized in C++ for given D,R at compile-time, but a more flexible approach is to allocate the neighbor ID and comp_vector arrays dynamically based on R in a contiguous block. However, dynamic-sized structs can’t be mapped zero-copy easily unless we treat the NodeBlock as a header followed by variable-length region. In practice, because R and D are known at index creation, we can code-generate or use templates for those sizes. We might also employ **std::span** or pointers in our in-memory struct that are adjusted to the mapped memory offsets. For example, if R differs, the offset to compressed vectors differs. As long as we know it from header, we can compute pointers accordingly. The `cista` library supports versioning and might allow different layouts for different index versions. A simple solution: store a version number and parameters in the file; if the code encounters a mismatched version, it knows it might not be able to interpret the data (or it can migrate it).

**Highway and C++20 Idioms:** We will use modern C++ to implement this design:

- **`std::atomic` and memory_order:** for the pointer swaps when installing new blocks, to ensure threads see a consistent update.
- **RAII (Resource Acquisition Is Initialization):** Use destructors to handle unpinning of pages. E.g., a class `MappedNode` that pins a node’s page in its constructor and unpins in destructor, so even if an exception occurs we don’t leave a page pinned.
- **`std::thread` or `std::jthread`:** for the sweeper thread. A `std::jthread` is nice as it can cooperate with stop tokens when the database is shutting down.
- **`<simd>`:** C++20’s `<experimental/simd>` could be used for portable SIMD in distance calculations, though Highway is an alternative. One might use `std::simd<float, 16>` to represent 16 floats and do arithmetic in bulk. However, since we have quantized ints, we might operate on `std::simd<int8_t, 64>` if available (for 512-bit AVX-512).
- **Filesystem and Memory mapping:** Using `std::filesystem` and OS APIs (POSIX `mmap` or Windows MapView) through either standard library or third-party to memory-map the file for zero-copy. Not strictly C++20, but well-supported.
- **`constexpr` and compile-time configuration:** We can make certain parameters `constexpr` (like block byte size, alignment) which allows the compiler to optimize the layout calculations at compile time and perhaps unroll loops for fixed sizes.
- **`std::variant` or strategy pattern:** Possibly to handle the different backlink storage strategies (A, B, C) we can have polymorphic behavior or compile-time flags. For example, if we want the same codebase to support both node-local and central index modes, we can abstract the neighbor update logic. However, this might complicate things; we might choose one strategy at implementation time.

By ensuring the on-disk format is basically a dump of in-memory structs, we minimize serialization overhead. In fact, during a checkpoint or shutdown, we could flush all dirty node blocks to disk in bulk via sequential writes – since our modifications happened in memory (either via memory map or via buffer cache), and they are already in the correct format, it’s just a matter of writing them out (the WAL ensures partial writes can be recovered).

Finally, **SIMD-friendly** also means being mindful of alignment and contiguous memory for bulk operations. We may allocate an array of NodeBlocks aligned to a page size, but inside each NodeBlock we might also pad to align segments. As mentioned, padding neighbor IDs section to a multiple of 32 bytes can ensure each compressed vector begins aligned. Given each compressed vector is 32 bytes in our example, this means each one is nicely aligned to begin with relative to the start of comp_vectors array; aligning the start of that array to 32 ensures alignment for all. This does waste a few bytes per node (padding), but 4 bytes out of 4KB is negligible (0.1%). It’s a worthy trade-off for speed.

To summarize, the storage format and code are crafted such that we **never need to copy or transform data during query processing** – we operate directly on memory-mapped bytes using CPU instructions (including SIMD). This maximizes throughput and minimizes CPU overhead, which is crucial given that disk-based ANN search is often I/O-bound; any CPU savings directly translates to handling more queries or doing more sophisticated healing in the background. Our design thus aligns with zero-copy principles: once data is written to disk in our NodeBlock format, it can be read and used as-is by the search algorithms.

## Conclusion and Implementation Outlook

In this design study, we outlined a comprehensive solution for maintaining a disk-based ANN graph index under dynamic updates in DuckDB. We evaluated different backlink storage layouts – from fully redundant in-block links to centralized indices – and recommended strategies to minimize write amplification while preserving search accuracy. We introduced a background sweeper mechanism that leverages MVCC epochs and WAL logging to safely garbage-collect deleted nodes and heal the graph’s connectivity in an atomic, crash-resilient manner. The edge healing algorithm connects neighbors of deleted nodes using a Vamana-inspired approach, ensuring the graph remains navigable and high-recall after massive deletions. Space reclamation policies will keep the index size in check by rewriting fragmented pages and recycling freed blocks, all without disrupting active queries.

Our emphasis on zero-copy, SIMD-friendly design means that the on-disk format doubles as the in-memory format, avoiding costly translations and allowing the use of modern C++20 and libraries like Highway for speed. Table-driven comparisons and pseudocode in this report serve as a blueprint for implementation in C++: using RAII for buffer management, atomic operations for pointer swaps, and careful alignment for vectorized computations. By following this design, DuckDB’s vector index (LM-DiskANN) will achieve **high performance** (both I/O and computation), **robustness** under continuous updates, and **stability** in search quality – meeting the demands of future applications where billions of vectors may be indexed on disk with frequent insertions and deletions.

**Sources:**

- Pan et al. (2023), *LM-DiskANN: Low Memory Footprint in Disk-Native Dynamic Graph-Based ANN Indexing*
- LibSQL Vector Index Blog – LM-DiskANN integration (2023)
- DuckDB Documentation (2025), MVCC and storage model
- FreshDiskANN (Aditi Singh et al. 2021) – dynamic ANN index approach
- Highway SIMD Library FAQ (2022) and documentation.