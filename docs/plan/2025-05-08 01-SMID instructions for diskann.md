Great. I’ll analyze how the Highway and SimSIMD libraries can improve your LM-DiskANN implementation for ternary quantization dot products and distance functions over float32, int8, and bit-packed ternary data. I’ll focus on SIMD support for AVX2, AVX-512, and NEON, with safe fallbacks, and how best to integrate or replace parts of your current `ternary_quantization.hpp`.

I'll get back to you shortly with a breakdown of primitives, performance tradeoffs, code references, and integration advice.


# Leveraging Highway and SimSIMD for LM‑DiskANN’s SIMD Kernels

## Background: Ternary Quantization and Current Implementation

**Ternary Vector Representation:** LM-DiskANN compresses float vectors into **bit-packed ternary** form with two parallel bit arrays (planes) per vector. The **positive plane** has a 1-bit wherever the original value was >0, and the **negative plane** has a 1-bit wherever the value was <0 (zero values set no bits). This yields 2 bits per dimension, stored in 64-bit words for alignment.

**Ternary Dot Product:** The similarity between a query vector and a database vector (in ternary form) is computed via a **dot product** on {-1,0,1} values. Given query positive bits `q⁺`, query negative `q⁻` (and similarly `v⁺`, `v⁻` for a database vector), the dot product is:

$\text{score} = \text{popcount}(q^+ \land v^+) - \text{popcount}(q^+ \land v^-) - \text{popcount}(q^- \land v^+) + \text{popcount}(q^- \land v^-) \,.$

This formula counts matching signs as +1 contributions and mismatched signs as –1. The current implementation realizes this by performing bitwise ANDs between the corresponding bit-planes and then computing population counts (popcount) of the resulting bit masks.

**SIMD Optimizations in Current Code:** The existing `ternary_quantization.hpp` includes specialized routines for different SIMD instruction sets:

* **AVX-512:** Uses the 64-bit *vector popcount* instruction (`VPOPCNTDQ`), via `_mm512_popcnt_epi64`, to count bits in 64-bit lanes in parallel. It processes 512 bits (8×64-bit words) per iteration and horizontally adds the results.
* **AVX2:** Lacks a dedicated vector popcount. The code falls back to a per-lane approach: it ANDs the 256-bit vectors and then uses scalar popcount on each 64-bit lane (using `__builtin_popcountll` or C++20 `std::popcount`). This is correct but relatively slow – a comment notes it’s *“potentially slower than optimized AVX2 popcount algorithms (e.g., Harley-Seal)”*.
* **NEON (ARM64):** Uses NEON’s 8-bit popcount instruction (`vcntq_u8`). Each 128-bit chunk is processed by counting bits per byte (`vcntq_u8`) and then using a horizontal pairwise sum (`vaddlvq_u8`) to get the total bits per 128-bit chunk. This avoids issues with saturating subtraction on unsigned bytes by summing counts before combining terms.
* **Scalar Fallback:** If none of those ISAs are available at runtime, a scalar loop computes popcounts (using `std::popcount` or compiler builtins).

At startup, the extension picks the best available kernel with a dispatch function (checking CPU flags). This ensures correctness on any CPU, but it means maintaining four versions of similar code. The AVX2 path in particular has a performance gap due to lack of vectorized popcount. There’s also duplicate logic (AND & popcount) written in multiple forms, which is a maintenance burden.

## Benefits of **Highway**: Portable SIMD with One Implementation

**What is Highway?** *Highway* is a C++ library for performance-portable SIMD operations developed by Google. It lets you write one set of vectorized code that can **target multiple architectures** (x86, ARM, RISC-V, etc.) by abstracting platform intrinsics. Highway supports dynamic dispatch – it can compile your function for AVX2, AVX-512, NEON, etc., and choose the best at runtime. In other words, *“the same application code can target various instruction sets”* and you can have the library include multi-versioned functions and pick the optimal one based on the CPU. This addresses the duplication in our current approach.

**Unified Ternary Dot Implementation:** Using Highway, we can write the ternary dot product logic once, and it will be compiled for each enabled target. For example, we would write something like:

```cpp
// Pseudocode using Highway
HWY_TARGET_FUNC int64_t TernaryDot(const uint64_t* qp, const uint64_t* qn,
                                   const uint64_t* vp, const uint64_t* vn, size_t words) {
  HWY_FULL(uint64_t) d;                   // Descriptor for full-width vector of 64-bit
  const size_t lanes = Lanes(d);          // Number of 64-bit lanes in this vector (depends on ISA)
  auto totalVec = Zero(d);                // vector accumulator (64-bit lanes)
  size_t i = 0;
  for (; i + lanes <= words; i += lanes) {
    auto qP = Load(d, qp + i);
    auto qN = Load(d, qn + i);
    auto vP = Load(d, vp + i);
    auto vN = Load(d, vn + i);
    // Bitwise ANDs
    auto and_qp_vp = And(qP, vP);
    auto and_qp_vn = And(qP, vN);
    auto and_qn_vp = And(qN, vP);
    auto and_qn_vn = And(qN, vN);
    // Popcount each 64-bit lane:
    auto cnt_qp_vp = PopCount(and_qp_vp); // hypothetical Highway PopCount per 64-bit lane
    auto cnt_qp_vn = PopCount(and_qp_vn);
    auto cnt_qn_vp = PopCount(and_qn_vp);
    auto cnt_qn_vn = PopCount(and_qn_vn);
    // Combine: term = cnt(qp&vp) - cnt(qp&vn) - cnt(qn&vp) + cnt(qn&vn)
    auto term = Sub(Sub(cnt_qp_vp, cnt_qp_vn), cnt_qn_vp);
    term = Add(term, cnt_qn_vn);
    totalVec = Add(totalVec, term);
  }
  // Horizontal sum of totalVec’s lanes to get final scalar (Highway provides SumOfLanes)
  int64_t total = 0;
  for (size_t lane = 0; lane < Lanes(d); ++lane) {
    total += GetLane(ExtractLane<0>(totalVec)); // (conceptual)
  }
  // Handle remainder words (if any) in scalar or smaller vectors...
  return total;
}
```

The above is conceptual (Highway API calls differ slightly), but it illustrates the approach. Highway provides `Load`/`Store`, `And`, `Add`, `Sub`, etc., which map to the appropriate intrinsic on each platform. For the horizontal sum, Highway offers utilities (e.g. `ReduceSum()` or converting to scalar) to avoid manual extraction. Key improvements and considerations:

* **Single Codepath:** This function, once written, can be compiled to AVX2, AVX-512, NEON, etc. using `HWY_DYNAMIC_DISPATCH` or by defining multiple targets. We eliminate separate `.avx2` and `.neon` functions – the Highway version handles all. As Jan Wassenberg (Highway author) notes, *“when using a wrapper such as Highway, you get exactly this kind of update after a recompile, or even just running your code on a CPU that supports newer instructions”*. In short, we stop writing AVX2-specific code; a recompile with AVX-512 enabled would automatically use 512-bit vectors and popcount if possible.

* **Popcount Implementation:** Highway does not (currently) have a single intrinsic like `_mm512_popcnt_epi64` abstracted, especially for AVX2 where no hardware popcount exists. We would need to implement popcount in terms of Highway operations. Several strategies are possible:

  * **Using builtins per lane:** We could simply call `std::popcount(x)` on each 64-bit element after extracting it. However, extracting each lane to scalar defeats vector parallelism (it’s essentially what our AVX2 fallback does now).
  * **Vectorized LUT:** A better approach is to use a lookup table to count bits in smaller chunks. For instance, Highway allows byte-level table lookups (`TableLookupBytes`) to implement a 4-bit or 8-bit popcount in parallel. We can create a 16-entry LUT for nibble popcounts (0–15) and use Highway to lookup each nibble of our 64-bit masks in parallel. This technique (a form of the classic LUT popcount) would let AVX2 process popcounts with SIMD operations on 16 or 32 bytes at a time, rather than scalar loop. (On AVX-512, if compiled with VPOPCNTDQ support, the compiler may emit that instruction when using a builtin popcount on 64-bit vectors, but we can also guard for that specifically).
  * **Harley-Seal algorithm:** For ultimate performance on AVX2, one could implement the Harley-Seal popcount method (a tree of SIMD operations and a small LUT). This is more complex, but Highway’s bitwise ops and shuffle capabilities (e.g., `Permute` or table lookup) are sufficient to implement it. Notably, this is exactly what a highly tuned AVX2 popcount would do. The benefit is we can implement it once in Highway, rather than in raw intrinsics with compiler intrinsics or assembly. (In fact, SimSIMD’s author added an AVX2 popcount using such techniques).

  Overall, even if we start with a simple method (like byte-wise LUT), Highway will give us a **correct, portable popcount** that works on all targets. We can later optimize it within the same code. By contrast, our current code’s AVX2 path is stuck with scalar popcounts. With Highway, we could close that gap – for example, using a vector LUT popcount would leverage AVX2’s 256-bit operations to count bits \~2x faster than scalar popcnt loops.

* **Vector Width and Alignment:** Highway’s API is *length-agnostic*. In AVX2 mode, `HWY_FULL(uint64_t)` might represent 4 lanes of 64-bit; in AVX-512 mode, 8 lanes; in NEON, 2 lanes. We don’t need to hardcode loop unroll factors like “process 8 words” or “4 words” – the loop uses `Lanes(d)` which the compiler sets appropriately. This makes the code future-proof if wider vectors (AVX512DQ, SVE) become available. Memory alignment is also handled gracefully: we can use `LoadU` (unaligned load) as shown, and Highway will choose the right instruction (on x86 it uses unaligned moves, on ARM it’s typically fine as is). Our current code already assumed 64-bit alignment for data, so using aligned loads could be an option too. Either way, Highway abstracts the difference between `_mm512_loadu_si512` and `vld1q_u8` etc.

* **Mask and Tail Handling:** In our current code, each SIMD function has to handle leftover elements (e.g., a tail of <8 64-bit words for AVX-512). Highway offers **masked loads/stores** and built-in tail processing. For example, it has facilities to generate a mask for “N remaining elements” and to perform partial operations. We could simplify the tail loop by using a masked load for the last partial vector. This was demonstrated in SimSIMD’s AVX-512 code using `_mm512_maskz_loadu_ps` for float remainders; Highway provides a similar abstraction for integers as well. In short, Highway can minimize scalar cleanup code.

* **Maintenance and Safety:** The logic for computing the dot product (ANDs and adds) is written once. It’s easier to read and verify than multiple intrinsics blocks, reducing the chance of bugs. We don’t have to manually handle different register widths or NEON’s lack of certain operations – Highway does that. This improves reliability. Also, if a new architecture (say RISC-V with vector popcount) emerges, the Highway devs may support it, and our code would *just work* after a recompile. Highway’s philosophy is that a wrapper can automatically take advantage of newer instructions, *“after a recompile, or even just running your code on a CPU that supports newer instructions”*. The trade-off is a slightly larger binary (embedding multiple code paths), but as Wassenberg notes, *“the cost is that the binary carries around both AVX2 and AVX-512 codepaths, but that is not an issue”* for most cases.

**Example – Encoding Optimization:** The encoding of floats to ternary bits can also benefit. Currently it likely loops over each float. With Highway, we could load (say) 16 floats at once (on AVX-512), compare them to 0 to get a mask of positive values and another mask for negatives, and then compress that mask to bits. Highway has mask manipulation functions; for instance, `MaskFromVec(Gt(v, 0.0f))` gives a mask, and `MaskBits(mask)` (or similar) yields an integer bitmask of 1s for true lanes. On AVX-512, a 16-bit mask corresponds to those 16 floats (like `_mm512_movemask_ps`). We could then store that mask directly as part of the bitplane words. Essentially, we process, say, 16 dimensions in one go instead of branching per dimension. On NEON, Highway would do a similar thing (Neon doesn’t have a single movemask instruction, but Highway’s implementation would handle it under the hood). This vectorized encoding could significantly speed up building the compressed index and ensures consistent behavior across platforms. (One must take care to combine multiple mask results to fill a 64-bit word – e.g., process in chunks of 64 if possible or accumulate partial masks – but it’s doable with Highway’s flexible permutes or by storing partial results and OR-ing them into the 64-bit word.)

**Performance Considerations:** With a proper Highway implementation, we expect **comparable performance to hand-tuned intrinsics** in many cases, and improved performance where the current code wasn’t optimal (e.g., AVX2 popcount). For example, if we implement a vectorized popcount for AVX2 via LUT or bit tricks, it could approach the speed of the known Harley-Seal algorithm (which is \~2× faster than naive scalar popcnt for large arrays). On AVX-512, our Highway code would essentially wrap the same instructions (it would use `_mm512_and_si512` and likely `_mm512_popcnt_epi64` internally if we use builtins – or a sequence of operations if not). The overhead of Highway itself is minimal – it’s header inline functions that compile down to the same machine code you’d write manually. In some cases, very low-level optimizations might be slightly harder to express; for instance, using the `_mm512_reduce_add_epi64` intrinsic directly is one instruction whereas Highway might do a pair of half-width reductions. But these differences are small, and the maintainers are continually improving the library to use optimal sequences. Given the complexity of our current multi-ISA code, the slight potential differences in cycle count are a worthwhile trade for clarity and future-proofing. And if needed, Highway allows embedding custom intrinsics for critical pieces (e.g., we could drop to `_mm512_popcnt_epi64` for the AVX-512 case explicitly while still keeping the rest portable).

**Summary of Highway Benefits:** By replacing custom intrinsics with Highway, we consolidate four versions of the ternary dot kernel into one, automatically support AVX-512 and NEON (and even SSE or future SVE) without extra code, and eliminate a lot of branching and duplicated logic. We gain *safety* (no CPU will run the wrong code path – Highway’s dynamic dispatch checks capabilities just like our `ResolveKernel`, and it’s well-tested) and *readability*. The main work is implementing an efficient popcount and any other bit-manipulations in a portable way, but that is a one-time cost with long-term payoff.

## Benefits of **SimSIMD**: Pre-Optimized Distance Primitives

**What is SimSIMD?** *SimSIMD* is a high-performance library specifically designed for fast vector similarity computations (dot products, distances) across languages (C, C++, Rust, Python, JavaScript, etc.). It provides a suite of functions like cosine similarity, inner product, Euclidean distance, Hamming distance, etc., optimized with SIMD for multiple architectures. SimSIMD acts as a collection of highly tuned kernels with **dynamic dispatch** internally. It supports AVX2, AVX-512 (including newer extensions like VNNI), NEON, SVE, etc., and falls back to scalar if needed. Notably, SimSIMD is already used in production systems – *“powering vector math in dozens of DBMS products and AI companies…deployed on well over 100 million devices”* – which speaks to its reliability.

**Plug-and-Play Kernels:** The biggest advantage of SimSIMD is that it offers **ready-made, optimized implementations** for exactly the kind of computations LM-DiskANN needs. We can replace custom code with calls into SimSIMD’s API:

* **Ternary Dot via Bit Operations:** While SimSIMD doesn’t have a one-call “ternary dot product” function, it has the building blocks:

  * It provides **binary popcount functions** for byte arrays. For example, in its Rust API one can do `u8::hamming(sliceA, sliceB)` to get the Hamming distance (popcount of A XOR B). Internally, that means SimSIMD can count bits in large bit vectors very quickly, using SIMD popcount instructions or emulations.
  * It likely exposes a direct popcount function as well (the Rust crate advertises a feature “popcnt – count number of set bits in a byte slice”). We can leverage this: to compute terms like popcount(qp & vp), we can perform the AND using normal C++ (which will be vectorized or at least very fast since it’s just memory bandwidth) and then call SimSIMD’s popcount on the resulting byte array. For instance, if each plane is an array of `N` bytes, and we compute an array `X` where each byte `X[i] = qp[i] & vp[i]`, then `SimdPopcount(X)` would return popcount of all bits in `X`. SimSIMD’s popcount will use the best available method (AVX-512 popcnt, AVX2 algorithm, NEON `vcnt`) under the hood – much better than a naive loop.
  * We need to do this for each of the four combinations (qp\&vp, qp\&vn, qn\&vp, qn\&vn). That’s four calls to a popcount routine over `N` bytes each. This overhead is negligible compared to doing it bit-by-bit; each call will internally process, say, 32 or 64 bytes at a time with SIMD. And since these AND operations are embarrassingly parallel, we could even structure it to interleave them for cache efficiency. But even done sequentially, it’s similar to what our code does (four AND+popcount loops) but now all four are heavily optimized.
  * Alternatively, SimSIMD provides a **Jaccard distance** for binary vectors, which computes popcount(A ∧ B) and popcount(A ∨ B). Jaccard and our ternary dot are different, but if SimSIMD internally computes intersection counts, we know it has an efficient way to get `popcount(A & B)`. In fact, SimSIMD’s author recommended using its Jaccard and Hamming implementations for binary vector metrics in other database contexts. So we’re standing on the shoulders of that work.
  * After obtaining the counts, combining them (`count_same - count_diff`) is just scalar arithmetic on the results (very cheap).

  The **net effect** is that we delegate all the *heavy lifting* (bit population counts across potentially thousands of 64-bit words) to SimSIMD. This would immediately resolve the AVX2 performance issue: SimSIMD’s popcount for AVX2 is not scalar per lane – it uses a vectorized method (the library maintainers implemented an AVX2 popcount in response to user requests). So, rather than our code doing one 64-bit popcount at a time, SimSIMD will process e.g. 256 bits at once using SIMD registers and perhaps a Harley-Seal approach. In other words, simply calling SimSIMD could **speed up the AVX2 case dramatically** (potentially 2× or more for that portion of the calculation). On AVX-512, SimSIMD will use `_mm512_popcnt_epi64` or a similarly optimal strategy. On ARM, it will use `vcntq_u8` and the like. We get the best known method on each platform automatically.

* **Float32 Distance Functions:** The LM-DiskANN extension also needs to compute distances in original (uncompressed) space for re-ranking or for uncompressed vectors (e.g., cosine similarity or inner product on 32-bit floats). SimSIMD has functions for this out-of-the-box. For example, one can call `simsimd_cosine_f32(a, b, length)` to get cosine similarity of two float arrays, or `simsimd_inner_f32` for raw dot product. These functions are highly optimized: SimSIMD’s AVX-512 implementation loads 16 floats at a time with mask handling for tails and uses `_mm512_fmadd_ps` (FMA) to do multiply-accumulates in parallel, exactly as one would hand-write. It then does horizontal reduction with `_mm512_reduce_add_ps`. On AVX2, it processes 8 floats at a time with `_mm256_dp_ps` or similar. On NEON, it would use `vmla` instructions. Critically, SimSIMD also handles the **norm computation** for cosine: it accumulates \$|a|^2\$ and \$|b|^2\$ alongside the dot and then does a reciprocal sqrt for the cosine formula. They even do this in double precision for final normalization to avoid precision differences across hardware. The result is a fast and *consistent* cosine similarity across all architectures (which can be important for reproducibility). By replacing our custom SIMD code with a call to `SimSIMD::cosine` or `SimSIMD::inner`, we instantly leverage these optimizations. We don’t have to worry about, say, AVX-512 masking or ARM’s lack of horizontal add – it’s taken care of.

* **Int8 / Quantized Distance:** If LM-DiskANN ever uses 8-bit quantized vectors (common in ANN for compression), SimSIMD supports that too. It has functions like `CosineI8` and `InnerI8` that treat signed 8-bit arrays as vectors and compute dot products or cosine. Under the hood, SimSIMD uses techniques like widening to 16-bit and using special instructions. On AVX-512 with VNNI, it will use the 8-bit dot product instruction (`VPDPBUSD`) which can multiply 64 bytes and add into 16 int32 accumulators in one go. The blog by SimSIMD’s author indicates they expected to see three `vpdpbusd` instructions for a 1536-dim cosine, which shows they aim to use these powerful instructions (the compiler sometimes needs nudging, but they handle it). On AVX2, where there is no single-step int8 multiply-add, SimSIMD implements a manual loop with `_mm256_madd_epi16` after unpacking bytes to int16, etc. All these details are encapsulated. We simply call, e.g., `simsimd_inner_i8(query, neighbor, dim)`, and get the dot product. The library ensures the accumulation is done in 32-bit or 64-bit to avoid overflow (their generated scalar fallback for i8 uses 32-bit accumulators for safety). So SimSIMD would not only maximize performance here, but also guarantee correctness for large dot products (no wrap-around).

* **Binary Hamming/Jaccard:** If in the future we needed pure binary vector comparisons (like if we use a binary hashing ANN), SimSIMD has those as well. As noted, `u8::hamming` gives Hamming distance, and `u8::jaccard` gives Jaccard distance. This indicates robust support for bit-vector ops. In our case, we mostly need popcounts of intersections/differences, which is essentially what Hamming/Jaccard provide. The *ClickHouse* project already decided to integrate SimSIMD for their bit-level distances for exactly this reason – rather than writing their own popcount with intrinsics, they call SimSIMD’s routine. We can do the same.

**Integration and Reliability:** Integrating SimSIMD into the DuckDB extension would involve adding it as a dependency (it’s available as a GitHub project and even as a submodule in some systems like ClickHouse). The API is C-friendly. For example, one might include a header and call a function like `simsimd_cosine_f32(const float* a, const float* b, size_t n)` which returns the cosine similarity. The library will perform a runtime dispatch internally to pick AVX-512 vs AVX2 vs scalar, similar to our `ResolveKernel` but likely more exhaustive. This means we **would not need our own dispatch logic at all** for those functions – we simply call the generic SimSIMD function each time, and it will use the fastest path. SimSIMD uses CPUID or OS mechanisms to detect instruction sets (in one of the HN threads, the author mentions dynamic dispatch with a function pointer table, which is analogous to what we do but handled inside the library). This reduces our code complexity (no maintenance of feature-detection code). It also ensures safety: if someone runs DuckDB on a CPU without AVX2, SimSIMD will automatically fall back to scalar code that’s been tested. We won’t accidentally call an AVX-512 intrinsic on an unsupported CPU – the library’s been designed to avoid that.

From a **reliability** standpoint, using SimSIMD is very attractive. It’s heavily tested across platforms (since it supports many languages, it likely has a comprehensive test suite to ensure each kernel returns correct results bit-for-bit). Also, because it is used in other database contexts (e.g., USearch, a vector search library, and even DuckDB’s own prior use via USearch), we can trust its correctness and performance in a similar setting. Memory safety is also a non-issue if we use it as intended: we provide pointers and lengths, and it internally will loop in a safe manner, handling any unaligned tails with masks (as evidenced by their use of masked loads for the last block in AVX-512 code). There’s no manual pointer arithmetic on our side except providing the right length.

**Performance Gains with SimSIMD:** In many cases, SimSIMD will be as fast as or faster than what we could achieve with custom intrinsics:

* On AVX-512, our custom code and SimSIMD will both effectively use the same instructions (popcnt, FMA, etc.). SimSIMD might add a tiny overhead for the function call and dispatch, but that is negligible for large vectors (the cost of scanning thousands of bytes dominates).
* On AVX2, SimSIMD’s advantage is clear: it replaces our scalar popcount loop with a vectorized method. The *Harley-Seal popcount* algorithm using AVX2 is known to *“be twice as fast as optimized popcnt-based function for large inputs”*. So we can expect a significant speedup in the ternary dot for CPUs that only have AVX2. The difference could directly translate to query latency improvement.
* On ARM NEON, our existing NEON code is already quite optimal (using vcnt). SimSIMD would likely do the same. But SimSIMD might also support ARM’s SVE2 (if available on newer processors), whereas our code currently doesn’t. So in the future, on a server-class ARM with SVE, SimSIMD could transparently use 256-bit or 512-bit vectors, giving a boost.
* For float32 operations, SimSIMD’s use of FMA and careful unrolling might outpace a naive intrinsics loop. The fact that it beat NumPy/BLAS in some benchmarks shows how much effort has gone into optimizing memory access patterns and instruction throughput. We could expect our cosine similarity computations to drop in latency by using SimSIMD (and it ensures multi-thread scaling if we offload queries, since it’s thread-safe and used in concurrent DBMS contexts).
* SimSIMD also often uses slightly advanced techniques like performing two operations in parallel to use wide registers fully. For instance, its Jaccard implementation might do some clever bit mixing. For our use-case, even if we just use the straightforward functions, we inherit those optimizations.

One concern might be the additional dependency, but SimSIMD is lightweight (essentially a collection of static inline/weak functions and some dispatch code). It’s already used in ClickHouse and others as a submodule, which suggests integration is manageable. In our DuckDB extension, we could include it similarly or link against it as an external library.

## Component-by-Component Comparison

To crystallize the differences, the table below compares how specific components of the ternary distance and related distance functions are implemented currently, and how they would be handled with Highway vs. SimSIMD:

| **Component / Operation**                  | **Current Custom Implementation**                                                                                                                                                                                                                                                                                                                                                                                                                              | **Using Highway SIMD**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | **Using SimSIMD Library**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| ------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Ternary dot – Popcount of 64-bit masks** | AVX-512: uses `_mm512_popcnt_epi64` for 8×64-bit lanes in parallel. AVX2: extracts each 64-bit lane to scalar and uses `__builtin_popcountll` (loop over 4 lanes per 256-bit). NEON: uses `vcntq_u8` on 128-bit, then sums bytes to get each 128-bit block’s bit count. Scalar fallback for others. Tail handling done by scalar loop in each case.                                                                                                            | Write a single loop with Highway `And` and a custom PopCount routine. For example, use `TableLookupBytes`: load a vector of bytes and use it to lookup in a 16-element nibble popcount LUT (via a single vector permute) – this yields popcount per byte in parallel. Sum bytes to 64-bit counts (Highway’s `SumOfLanes` or pairwise adds). Highway would utilize 256-bit registers on AVX2 and 512-bit on AVX-512 automatically. If VPOPCNTDQ is available, using `std::popcount` on a 64-bit mask may compile down to that instruction, achieving parity with our AVX-512 code. The AVX2 path would be much faster than the current scalar-lane method (using \~4× 64-bit operations in parallel). On NEON, Highway would use `vcnt` under the hood for byte popcount if we do the LUT method (NEON can also do table lookups via `vtbl` which Highway abstracts). In all cases, the code is identical – Highway picks optimal instructions per platform. Tail elements can be handled by Highway’s masked load or a scalar loop; either way we don’t need separate logic per ISA. | Use SimSIMD’s optimized bit-count functions. For example, compute four bit masks: `mask1 = qp & vp` (as a byte array), etc. Then call SimSIMD’s popcount function on each mask. e.g. `size_t c1 = simsimd_popcnt_u8(mask1, length_in_bytes)`. This returns popcount of all bits in that mask. Do this for c2, c3, c4 and compute `result = (c1 - c2 - c3 + c4)`. Each `popcnt_u8` call internally dispatches to the best routine: on AVX-512 it uses `_mm512_popcnt_epi64` or equivalent; on AVX2 it uses a vectorized algorithm (LUT or Harley-Seal); on NEON it uses `vcntq_u8`. We leverage all those optimizations by a simple function call. No manual loop needed on our side, and the library handles alignment and remainder bits safely.                                                                                                                                  |
| **Bitwise operations (AND, XOR)**          | Intrinsics for each ISA: `_mm512_and_si512` / `_mm256_and_si256` / `vandq_u8` etc., written in each version of the code. XOR (if needed for other distances) likewise with `_mm256_xor_si256`, etc.                                                                                                                                                                                                                                                            | Highway overloads bitwise ops for vectors. We can do `auto x = And(a, b)` or simply `a & b` on Highway vectors. This works for 64-bit, 32-bit, 8-bit vectors alike. Highway ensures the right intrinsic is used (e.g., on NEON it will use `vandq_u64` for 64-bit). No separate code paths – the same C++ code handles all. These operations are inlined and incur no overhead.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | SimSIMD’s high-level API doesn’t expose bitwise ops directly (it focuses on complete metrics). We can just use C++ builtins for AND/XOR on raw data before calling SimSIMD. For example, use `_mm256_stream_load_si256` or even simple loop to AND two bit arrays (which will likely auto-vectorize). However, this extra step is often unnecessary: SimSIMD’s functions like Hamming or Jaccard take original data and do XOR/AND internally. If we use those (e.g., use Hamming for `(qp⊕vp)` to get popcount of differences), we skip doing it ourselves. In short, SimSIMD tends to encapsulate the bit ops needed for distance calculations.                                                                                                                                                                                                                                  |
| **Vector loads & memory alignment**        | Uses unaligned loads (`_mm512_loadu_si512`, `_mm256_loadu_si256`) for bitplanes, since data might not be 64-byte aligned. NEON uses `vld1q_u8` (which requires 16-byte alignment; code assumes `uint64_t` alignment which is enough). The code aligns local buffers (like the 256-bit temp popcount storage) with `alignas(32)` manually.                                                                                                                      | Highway provides `Load(d, ptr)` and `LoadU(d, ptr)` for aligned/unaligned loads. Given our data is 8-byte aligned (and likely 32 or 64-byte aligned due to allocation), we could use aligned loads for a slight benefit, but unaligned is safer and generally just as fast on modern x86. Highway will choose the appropriate instruction. Stores and intermediate vectors are handled similarly with `Store`. We no longer need to manually ensure alignment of temp arrays; Highway’s stack-allocated vectors (if any) will be suitably aligned. Overall, memory access code is simpler and less error-prone.                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | SimSIMD’s API deals with raw pointers and lengths. Internally, it will use aligned loads when possible. For example, its AVX-512 kernel uses `_mm512_maskz_loadu_ps` for float which handles unaligned remainders. As users, we don’t manage SIMD loads at all – we hand the pointer to SimSIMD and it performs the loads inside its function. We just need to ensure the data buffers (bitplanes or float arrays) are properly allocated (which they are, coming from our index storage). SimSIMD functions likely handle odd-length cases by masking or scalar loops internally, so we won’t segfault even if length isn’t a multiple of the vector width.                                                                                                                                                                                                                       |
| **Horizontal summation of lanes**          | AVX-512: uses `_mm512_reduce_add_epi64` to sum 8 lanes to one scalar. AVX2: stores 256-bit result to an array and sums 4 int64 values in scalar. NEON: accumulates counts into a scalar in the loop (no 64-bit horizontal add available). Each kernel has its own reduction method.                                                                                                                                                                            | Highway offers `SumOfLanes(vector)` or similar, which returns the sum across all lanes. We can use that to get the final dot product from the vector accumulator. Internally, it will use the optimal reduction: on AVX-512, it might use a single reduce instruction; on AVX2, it might do pairwise adds (or even use SSE’s `_mm_hadd_epi64` in a cascade). The key is we don’t have to write it – one line replaces those multiple #ifdef’d sections. If needed, we could also extract lanes manually with `GetLane()` for small lane counts, but built-in reduce is clearer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | SimSIMD returns final results directly for its high-level functions. For popcount of a full array, it produces a scalar count. For dot products, it produces the dot or cosine as a single float. So there’s effectively no manual horizontal sum needed – the library’s function already did the summation (often using vector reduce add instructions inside). For example, `u8::hamming(a,b)` in SimSIMD will internally sum all 8-bit popcounts into one integer result. This simplifies our code – we don’t accumulate partially and then finish off in C++, we get the answer in one call.                                                                                                                                                                                                                                                                                   |
| **Float32 dot or cosine**                  | Likely uses an inner loop with intrinsics (not shown in snippet). E.g., AVX2 might use `_mm256_dp_ps` or do 8-wide FMA accumulations; AVX-512 does 16-wide. The design doc mentions “SIMD intrinsics over std::span<float> data” for cosine/IP. So custom code handles normalization (computing vector norms) and dot products for each ISA, plus scalar fallback.                                                                                             | Highway can implement a dot product in a straightforward manner. Example: `for (; i < n; i += Lanes(d)) { auto va=Load(d,a+i); auto vb=Load(d,b+i); sumVec = MulAdd(va, vb, sumVec); }`. After the loop, use `SumOfLanes(sumVec)` to get the dot. For cosine, we’d simultaneously accumulate `sumVec`, `sumA2` (`va*va`), and `sumB2` (`vb*vb`) using `MulAdd`. At the end, we’d do `invLen = 1.0f / sqrt(SumOfLanes(sumA2) * SumOfLanes(sumB2))` and multiply by `SumOfLanes(sumVec)`. Highway will ensure FMA is used on platforms that support it, and the loop will be unrolled for the specific vector width. This single function replaces separate SSE/AVX/AVX-512 implementations. The result might have minor floating-point differences from our current code (depending on reduction order), but we can mitigate that by accumulating in double if needed, similar to SimSIMD’s approach (Highway supports promoting to wider types easily).                                                                                                                              | SimSIMD provides `simsimd_cosine_f32` and `simsimd_inner_f32` (and similar for f64, f16). We can call those directly. For example, `float cosine = f32::cosine(query, neighbor, dim)`. Under the hood, SimSIMD will call the best implementation: AVX-512 path does masked loads and 16-wide FMAs, AVX2 path likely does 8-wide FMAs or two 4-wide operations, etc. It also does the normalization internally (the return is the cosine similarity, not just the dot). SimSIMD likely uses high precision for the norm to avoid rounding issues (their AVX-512 code converts the final floats to double for `rsqrt` calculation). This means by using SimSIMD we get a very robust cosine implementation with virtually no effort. No need to worry about precision corner cases or ISA-specific normalization.                                                                    |
| **Int8 / int16 distance**                  | If implemented, would require manual handling: e.g., for int8 vectors, use `_mm256_cvtepi8_epi16` to extend 32×int8 -> 32×int16, multiply pairs to 16-bit results, accumulate into 32-bit. AVX-512 could use `_mm512_dpbusd_epi32` (if VNNI available) or `_mm512_madd_epi16` after extending to int16. This is complex and not in the current code (ternary uses bits instead). Int16 (if used for other quantization) similarly needs careful SIMD handling. | Highway can manage integer vectors too. For int8 dot: one would use `PromoteTo(int16, v8)` to get two int16 vectors (for low and high halves of bytes), multiply by corresponding promoted halves of the other vector, then accumulate into 32-bit. Highway has widening multiply (e.g., `MulEven` and `MulHigh` for 16-bit multiplies producing 32-bit). It doesn’t directly expose the VNNI instruction, but if compiled with AVX-512VNNI, the compiler might fuse a pattern into `VPDPBUSD`. Still, achieving peak int8 performance with Highway might require some low-level tuning (or waiting for Highway to add a dedicated dot function). So it’s possible, but more involved than floats.                                                                                                                                                                                                                                                                                                                                                                                   | SimSIMD is *especially* strong here: it was designed to handle quantized vectors efficiently. By calling `i8::inner(a,b)` or `i8::cosine(a,b)`, we get a fully optimized routine. For example, SimSIMD’s AVX2 int8 cosine uses clever combinations of `_mm256_maddubs_epi16` and `_mm256_madd_epi16` to sum 32 products at a time, and it unrolls the loop to cover the whole vector length. On AVX-512 with VNNI, it will utilize `VPDPBUSD` (8-bit dot product) – in fact, this was a highlight of their optimization efforts. They’ve reported huge speedups (e.g., 133× vs Python for int8 cosine on large vectors). For us, using SimSIMD means we don’t have to implement any of this; we simply call into these routines and get near peak hardware throughput for int8 dot products. This is a big win if we introduce product quantization or other int8 data in DiskANN. |

**Notes:** Both Highway and SimSIMD would also help with any **L2 (Euclidean) distance** computations (not heavily discussed in the prompt but mentioned in design doc as a possibility). Highway can vectorize the (a-b)^2 sum easily, and SimSIMD actually has a `sqeuclidean` function in its API (which computes \$\sum (a\_i - b\_i)^2\`). So whichever route we choose, future distance metrics can be handled.

## Trade-offs: Custom Intrinsics vs. Highway vs. SimSIMD

Choosing between writing our own intrinsics, using Highway, or using SimSIMD involves trade-offs in performance, safety, and maintainability:

* **Performance:** All approaches aim to use SIMD, but SimSIMD is likely to yield the best out-of-the-box performance for our specific needs. It has been *hand-optimized for dot products and distances* on each platform, often using techniques beyond what a compiler autovectorizer or a straightforward abstraction would use (like combining operations, using newer instructions, etc.). For example, SimSIMD’s use of AVX-512 VNNI for int8 and its AVX2 popcount optimizations means it can outperform a naive SIMD loop. Highway, on the other hand, will generate very efficient code, but might not always use the absolute newest trick unless we implement it. For instance, Highway code performing an int8 dot will be fast, but might not quite match the throughput of `VPDPBUSD` unless we explicitly handle that case or rely on compiler pattern recognition. However, for the *ternary popcount and float operations*, Highway can achieve parity with intrinsics in most cases – after all, it will use the same instructions (if we implement the popcount wisely). A custom intrinsic implementation gives maximal control: we could micro-optimize every instruction if we had unlimited time. But maintaining that across architectures is arduous. It’s worth noting that in our current state, the *bottlenecks are not fully optimized* (e.g., AVX2 popcount). SimSIMD would immediately fix those, whereas with Highway we’d have to put in some effort (which is still much less than writing separate intrinsics, but not zero). In summary: **SimSIMD offers plug-and-play high performance**, likely exceeding our current performance on all CPUs. Highway offers *very good performance with easier maintainability*, and any small gap can often be closed with minor tweaks or future improvements (and you get performance boosts by simply recompiling on a new CPU or updating Highway). Custom code can be as fast as theoretically possible, but as we see, it’s easy to overlook cases (like AVX2 popcount) or not utilize new instructions (our AVX2 code doesn’t use BMI2/PEXT or VPCLMUL for popcount, etc., which SimSIMD or a future Highway might).

* **Safety and Correctness:** Both Highway and SimSIMD significantly reduce the risk of bugs. Our custom code has to deal with subtle points: e.g., the NEON code had to avoid unsigned wrap in subtract by doing summation first – an insight that if missed could yield wrong results for certain bit patterns. When using a high-level library, these details are handled internally by experts. SimSIMD especially emphasizes numerically consistent results (for float) across platforms by using higher precision where needed, so we avoid discrepancies. Highway ensures we won’t accidentally use an undefined intrinsic or read past the end of an array (with masked loads, it handles tails safely). It also avoids mistakes like forgetting a `#if` guard on one of the code paths. Our current code is quite careful, but as it grows (imagine adding SSE4 or AVX512VBMI paths), the complexity could introduce bugs. SimSIMD and Highway have already been through that testing. Additionally, using these libraries means we benefit from continuous fixes: if an issue is found (e.g., a particular CPU behaves oddly with an instruction), the maintainers will address it in the library, and upgrading it fixes our code too. With custom intrinsics, we’d have to discover and fix such issues ourselves.

* **Future-proofing:** Using Highway or SimSIMD means **easier adoption of future hardware advances**. If Intel releases AVX512VPOPCNTDQ (which they have) or ARM releases SVE with popcount, our custom code would need manual updating to use those. Highway already supports AVX512VPOPCNTDQ (we used it) and could support SVE down the line; in fact, Highway explicitly supports scalable vectors (like RISC-V’s V extension) in a single codebase. SimSIMD already supports SVE and AVX-512 VNNI, etc., and the author is clearly keeping it up with latest tech (e.g., trying to accelerate Jaccard further with new instructions). This means if, say, next-gen CPUs add a “ternary dot product” instruction (hypothetically), SimSIMD might incorporate it and our code would speed up without a rewrite. Highway would allow us to recompile and benefit if the compiler maps our operations to that instruction. By contrast, a custom solution tends to stagnate – e.g., our AVX2 code never got the Harley-Seal improvement until someone specifically does it, whereas a library might do that proactively.

* **Maintainability & Development Speed:** Rewriting four versions of a function whenever we make a change (like to support a new metric or fix a bug) is time-consuming. With Highway, we maintain one implementation, which is clearly a win. With SimSIMD, we might not maintain that code at all – we simply call library functions. This frees us to focus on higher-level logic. If we needed to implement a new distance metric, with Highway we implement it once (not N times). With SimSIMD, we first check if it’s already provided (many are); if not, we might have to implement it either via combining existing library calls or falling back to Highway or scalar for that one. In general, SimSIMD covers the common metrics; if we stick to those, maintainability is extremely high (just use the library). One risk of external libraries is dependency management, but since SimSIMD is header-only or a small static lib and is permissively licensed (MIT/Apache), it should integrate smoothly. Highway is header-only (Apache-2 license) as well – also easy to bundle or require as part of our extension.

* **Flexibility:** Highway gives us full control to implement *any* operation in SIMD (not just pre-defined patterns). This is useful for the ternary dot, which is a custom combination of popcounts. We can implement that in Highway as we need. SimSIMD, being more specialized, doesn’t have an explicit ternary function – we have to assemble it from popcount calls. That’s still fairly straightforward, but it’s a bit of a “custom” use of SimSIMD. If tomorrow we devised a new compression scheme (say 4-bit quantization), Highway would let us implement its distance function readily. SimSIMD might not directly support 4-bit (unless it’s common enough, e.g., it might not have a dedicated function for 3-value dot products since that’s niche). **However**, SimSIMD does cover int8 and binary, which often can be building blocks for other schemes (e.g., one could encode 2-bit values as bit strings or 4-bit values as two int8 half-bytes and use a LUT approach). Overall, for highly custom logic, Highway is the more flexible tool; SimSIMD handles *standard distance computations* with unbeatable ease.

* **Dependency vs. Self-contained:** Adopting SimSIMD means our extension relies on an external library for core calculations. This is generally fine (the library is open-source and can be statically linked or included), but some projects prefer minimizing dependencies. Highway, being more of a utility library, might be more palatable as just an internal implementation detail (it’s header-only, so it doesn’t feel like an external dependency). If DuckDB already uses SimSIMD via USearch (as hinted on HN), then adding a direct usage is natural and not a concern. If not, one must consider whether to introduce it. Given its advantages, it likely justifies its inclusion.

**In conclusion**, both Highway and SimSIMD would greatly improve the **performance** and **reliability** of the LM-DiskANN extension’s SIMD routines.

* *Highway* would let us write a clear, single implementation of the ternary dot and any other distance function, covering AVX2, AVX-512, NEON, etc., with safe fallbacks. It would remove architecture-specific sections and thereby reduce bugs and maintenance effort. We would need to implement efficient popcount and a few bit-manipulation tricks within Highway, but once done, we’d get consistent high performance. Highway’s maintainers are active, so as new CPUs come out, we could recompile or update Highway to use new features (e.g., if one day RISC-V servers become relevant, Highway could make our code run with vector acceleration there too). The code would be easier for others to understand (no maze of ifdefs and compiler attributes).

* *SimSIMD* offers an even higher-level approach: use battle-tested, specialized primitives for all our critical computations. The ternary dot can be constructed from SimSIMD’s binary popcount and similarity functions with just a few lines, and the library will handle all low-level optimizations. Our float32 and int8 distance calculations can essentially defer entirely to SimSIMD one-liners, ensuring we use the state-of-the-art methods (on par with or better than MKL/BLAS for floats, and far better than naive methods for int8). This drastically reduces the amount of SIMD code we write ourselves – which is great for long-term **reliability** (less code to get wrong). Whenever a new optimization or instruction comes out, updating the SimSIMD version could yield instant benefits (for example, if a new popcount method is found, they might incorporate it and we’d speed up without changing our code). The flip side is a slight loss of low-level control – but for our known use-cases, SimSIMD covers them comprehensively.

Given that DuckDB and related projects already lean toward using SimSIMD for vector search, that route seems very compelling. We could also adopt a **hybrid approach**: use SimSIMD for the mainstream distance computations (float32, int8, binary) and possibly use Highway (or manual SIMD) for any niche parts not directly covered by SimSIMD (though ternary popcount is so simple to achieve with SimSIMD that a full Highway rewrite might be unnecessary). Either way, both libraries would make the extension faster and easier to maintain compared to the status quo, where we juggle multiple intrinsic versions by hand.

---

**References:**

* LM-DiskANN Ternary encoding and current SIMD strategy
* Highway library documentation on portability and dispatch
* SimSIMD usage examples and discussions
* ClickHouse issue referencing SimSIMD for popcount and Jaccard distance
* SimSIMD’s design highlighting multi-architecture support and optimizations.
